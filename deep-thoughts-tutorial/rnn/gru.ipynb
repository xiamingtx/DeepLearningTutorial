{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 实现GRU网络\n",
    "\n",
    "关于GRU和LSTM这些门网络的选择, 参考:\n",
    "\n",
    "[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "lstm_layer = nn.LSTM(3, 5)\n",
    "gru_layer = nn.GRU(3, 5)\n",
    "\n",
    "print(sum(p.numel() for p in lstm_layer.parameters()))\n",
    "print(sum(p.numel() for p in gru_layer.parameters()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在input_size相同的情况下 GRU的参数数目大致是LSTM的$\\frac{3}{4}$倍\n",
    "\n",
    "### 实现GRU"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def gru_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh):\n",
    "    prev_h = initial_states\n",
    "    bs, T, i_size = input.shape\n",
    "    h_size = w_ih.shape[0] // 3\n",
    "\n",
    "    # 对权重扩维 复制成batch_size倍\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1)\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)\n",
    "\n",
    "    output = torch.zeros(bs, T, h_size)  # GRU网络的输出状态序列\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:, t, :]  # t时刻GRU cell的输入特征向量, [bs, i_size]\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))  # [bs, 3*h_size, 1]\n",
    "        w_times_x = w_times_x.squeeze(-1)  # [bs, 3*h_size]\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))  # [bs, 3*h_size, 1]\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)  # [bs, 3*h_size]\n",
    "\n",
    "        # 重置门\n",
    "        r_t = torch.sigmoid(w_times_x[:, :h_size] + w_times_h_prev[:, :h_size]\n",
    "                            + b_ih[:h_size] + b_hh[:h_size])\n",
    "        # 更新门\n",
    "        z_t = torch.sigmoid(w_times_x[:, h_size:2 * h_size] + w_times_h_prev[:, h_size:2 * h_size]\n",
    "                            + b_ih[h_size:2 * h_size] + b_hh[h_size:2 * h_size])\n",
    "        # 候选状态\n",
    "        n_t = torch.tanh(w_times_x[:, 2 * h_size:3 * h_size] + b_ih[2 * h_size:3 * h_size] +\n",
    "                         r_t*(w_times_h_prev[:, 2 * h_size:3 * h_size] + b_hh[2 * h_size:3 * h_size]))\n",
    "        prev_h = (1 - z_t) * n_t + z_t * prev_h  # 增量更新隐含状态\n",
    "        output[:, t, :] = prev_h\n",
    "\n",
    "    return output, prev_h"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1593,  0.7147, -1.0306, -0.1275,  0.3484],\n",
      "         [-0.3227, -0.0368, -0.5100,  0.4591, -0.1904],\n",
      "         [ 0.0651,  0.2047, -0.3852,  0.1842,  0.1674]],\n",
      "\n",
      "        [[ 0.4132, -0.6484, -0.1507, -0.4509,  1.4405],\n",
      "         [ 0.0431, -0.4333, -0.1946,  0.1758,  1.0490],\n",
      "         [ 0.1021, -0.1294,  0.0391,  0.4045,  0.6429]]],\n",
      "       grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "\n",
    "input = torch.randn(bs, T, i_size)  # 输入序列\n",
    "h0 = torch.randn(bs, h_size)\n",
    "\n",
    "# 调用PyTorch官方的GRU API\n",
    "gru_layer = nn.GRU(i_size, h_size, batch_first=True)\n",
    "output, h_final = gru_layer(input, h0.unsqueeze(0))\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([15, 4])\n",
      "weight_hh_l0 torch.Size([15, 5])\n",
      "bias_ih_l0 torch.Size([15])\n",
      "bias_hh_l0 torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "for k, v in gru_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1593,  0.7147, -1.0306, -0.1275,  0.3484],\n",
      "         [-0.3227, -0.0368, -0.5100,  0.4591, -0.1904],\n",
      "         [ 0.0651,  0.2047, -0.3852,  0.1842,  0.1674]],\n",
      "\n",
      "        [[ 0.4132, -0.6484, -0.1507, -0.4509,  1.4405],\n",
      "         [ 0.0431, -0.4333, -0.1946,  0.1758,  1.0490],\n",
      "         [ 0.1021, -0.1294,  0.0391,  0.4045,  0.6429]]], grad_fn=<CopySlices>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 调用自定义的gru_forward函数\n",
    "\n",
    "output_custom, h_final_custom = gru_forward(input, h0, gru_layer.weight_ih_l0, gru_layer.weight_hh_l0,\n",
    "                                            gru_layer.bias_ih_l0, gru_layer.bias_hh_l0)\n",
    "print(output_custom)\n",
    "print(torch.allclose(output, output_custom))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9146fa6d",
   "language": "python",
   "display_name": "PyCharm (DeepLearningTutorial)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}