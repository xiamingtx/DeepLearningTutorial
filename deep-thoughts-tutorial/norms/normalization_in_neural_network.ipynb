{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Batch Norm\n",
    "statistic term\n",
    "\n",
    "NLP: [N, L, C] -> [C]\n",
    "\n",
    "CV: [N, C, H, W] -> [C]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.8588,  0.8554,  0.5678,  0.6646],\n         [ 0.3913, -0.5461,  0.7717,  2.9826],\n         [-0.1380,  0.3573,  0.3497,  0.8792]],\n\n        [[ 0.8653,  0.4413, -1.6207,  0.2205],\n         [-0.8848,  0.6860, -0.6809,  0.9544],\n         [-0.0402, -0.3383,  1.3893,  0.1175]]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, time_steps, embedding_dim = 2, 3, 4\n",
    "eps = 1e-5\n",
    "num_groups = 2\n",
    "\n",
    "inputx = torch.randn(batch_size, time_steps, embedding_dim)\n",
    "inputx  # N * L * C"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.7595,  1.1925,  0.4398, -0.3205],\n         [ 0.0571, -1.5348,  0.6444,  2.1137],\n         [-0.5569,  0.2232,  0.2209, -0.0952]],\n\n        [[ 0.6069,  0.3867, -1.7560, -0.7868],\n         [-1.4232,  0.8629, -0.8131, -0.0162],\n         [-0.4434, -1.1305,  1.2640, -0.8950]]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_norm_op = nn.BatchNorm1d(embedding_dim, affine=False)\n",
    "batch_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.7595,  1.1925,  0.4398, -0.3205],\n         [ 0.0571, -1.5348,  0.6444,  2.1137],\n         [-0.5569,  0.2232,  0.2209, -0.0952]],\n\n        [[ 0.6069,  0.3867, -1.7560, -0.7868],\n         [-1.4232,  0.8629, -0.8131, -0.0162],\n         [-0.4434, -1.1305,  1.2640, -0.8950]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_mean = inputx.mean(dim=(0, 1), keepdim=True)\n",
    "bn_std = inputx.std(dim=(0, 1), keepdim=True, unbiased=False)\n",
    "\n",
    "(inputx - bn_mean) / (bn_std + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layer Norm\n",
    "常用于NLP\n",
    "\n",
    "statistic term\n",
    "\n",
    "NLP: [N, L, C] -> [N, L]\n",
    "\n",
    "CV: [N, C, H, W] -> [N, H, W]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.6966, -0.2553, -0.8148, -0.6265],\n         [-0.3929, -1.1169, -0.0990,  1.6088],\n         [-1.3900, -0.0132, -0.0343,  1.4375]],\n\n        [[ 0.9346,  0.4887, -1.6798,  0.2565],\n         [-1.1149,  0.8235, -0.8633,  1.1547],\n         [-0.4884, -0.9401,  1.6779, -0.2494]]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm_op = nn.LayerNorm(embedding_dim, elementwise_affine=False)\n",
    "layer_norm_op(inputx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.6966, -0.2553, -0.8148, -0.6265],\n         [-0.3929, -1.1169, -0.0990,  1.6088],\n         [-1.3900, -0.0132, -0.0343,  1.4375]],\n\n        [[ 0.9346,  0.4887, -1.6798,  0.2565],\n         [-1.1149,  0.8235, -0.8633,  1.1547],\n         [-0.4884, -0.9401,  1.6779, -0.2494]]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_mean = inputx.mean(dim=-1, keepdim=True)\n",
    "ln_std = inputx.std(dim=-1, keepdim=True, unbiased=False)\n",
    "(inputx - ln_mean) / (ln_std + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instance Norm\n",
    "常用于风格迁移上\n",
    "\n",
    "statistic term\n",
    "\n",
    "NLP: [N, L, C] -> [N, C]\n",
    "\n",
    "CV: [N, C, H, W] -> [N, C]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.3671,  1.0916,  0.0274, -0.8072],\n         [-0.3702, -1.3244,  1.2106,  1.4092],\n         [-0.9969,  0.2329, -1.2380, -0.6020]],\n\n        [[ 1.2387,  0.4082, -1.0471, -0.5643],\n         [-1.2103,  0.9685, -0.2997,  1.4051],\n         [-0.0284, -1.3767,  1.3468, -0.8408]]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins_norm_op = nn.InstanceNorm1d(embedding_dim)\n",
    "ins_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.3671,  1.0916,  0.0274, -0.8072],\n         [-0.3702, -1.3244,  1.2107,  1.4092],\n         [-0.9969,  0.2329, -1.2382, -0.6020]],\n\n        [[ 1.2387,  0.4082, -1.0471, -0.5643],\n         [-1.2103,  0.9685, -0.2997,  1.4051],\n         [-0.0284, -1.3767,  1.3468, -0.8408]]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_mean = inputx.mean(dim=1, keepdim=True)\n",
    "in_std = inputx.std(dim=1, keepdim=True, unbiased=False)\n",
    "(inputx - in_mean) / (in_std + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group Norm\n",
    "\n",
    "statistic term\n",
    "\n",
    "NLP: [N, G, L, C // G] -> [N, G]\n",
    "\n",
    "CV: [N, G, C // G, H, W] -> [N, G]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.8279,  0.5137, -0.5283, -0.4190],\n         [-0.0940, -1.3217, -0.2982,  2.1967],\n         [-0.7873, -0.1386, -0.7744, -0.1769]],\n\n        [[ 1.2214,  0.5252, -1.6883,  0.1576],\n         [-1.6528,  0.9271, -0.7461,  0.8933],\n         [-0.2656, -0.7552,  1.3293,  0.0543]]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_norm_op = nn.GroupNorm(num_groups, embedding_dim, affine=False)\n",
    "group_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.8279,  0.5137, -0.5282, -0.4190],\n         [-0.0940, -1.3217, -0.2981,  2.1967],\n         [-0.7873, -0.1386, -0.7744, -0.1769]],\n\n        [[ 1.2214,  0.5252, -1.6883,  0.1576],\n         [-1.6528,  0.9271, -0.7461,  0.8933],\n         [-0.2656, -0.7552,  1.3293,  0.0543]]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_inputxs = torch.split(inputx, split_size_or_sections= embedding_dim // num_groups, dim=-1)\n",
    "results = []\n",
    "for g_inputx in group_inputxs:\n",
    "    gn_mean = g_inputx.mean(dim=(1, 2), keepdim=True)\n",
    "    gn_std = g_inputx.std(dim=(1, 2), keepdim=True, unbiased=False)\n",
    "    gn_result = (g_inputx - gn_mean) / (gn_std + eps)\n",
    "    results.append(gn_result)\n",
    "torch.cat(results, dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weight Norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.3235,  0.5420, -0.6963],\n         [-1.1378, -0.4317, -0.9247],\n         [-0.6015, -0.1038, -0.3725]],\n\n        [[ 0.6753,  1.1038, -0.9312],\n         [-0.8022,  0.2282, -0.7469],\n         [-0.3648, -0.7307,  0.5381]]], grad_fn=<UnsafeViewBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(embedding_dim, 3, bias=False)\n",
    "wn_linear = nn.utils.weight_norm(linear)\n",
    "wn_linear(inputx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.3235,  0.5420, -0.6963],\n         [-1.1378, -0.4317, -0.9247],\n         [-0.6015, -0.1038, -0.3725]],\n\n        [[ 0.6753,  1.1038, -0.9312],\n         [-0.8022,  0.2282, -0.7469],\n         [-0.3648, -0.7307,  0.5381]]], grad_fn=<MulBackward0>)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_direction = linear.weight / linear.weight.norm(dim=1, keepdim=True)\n",
    "weight_magnitude = wn_linear.weight_g\n",
    "inputx @ (weight_direction.transpose(-1, -2)) * (weight_magnitude.transpose(-1, -2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "关于权重归一化的再次说明"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "batch_size, feat_dim, hid_dim = 2, 3, 4\n",
    "inputx = torch.randn(batch_size, feat_dim)\n",
    "linear = nn.Linear(feat_dim, hid_dim, bias=False)\n",
    "wn_linear = nn.utils.weight_norm(linear)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight:\n",
      "tensor([[-0.2551, -0.4248, -0.5152],\n",
      "        [ 0.2330,  0.1396,  0.2011],\n",
      "        [-0.2154,  0.2437,  0.5582],\n",
      "        [-0.1916,  0.1428,  0.0081]], grad_fn=<WeightNormInterfaceBackward0>)\n",
      "weight_magnitude:\n",
      "tensor([[0.7148],\n",
      "        [0.3380],\n",
      "        [0.6460],\n",
      "        [0.2391]])\n",
      "weight_direction:\n",
      "tensor([[-0.3569, -0.5943, -0.7208],\n",
      "        [ 0.6895,  0.4129,  0.5951],\n",
      "        [-0.3335,  0.3772,  0.8640],\n",
      "        [-0.8014,  0.5972,  0.0338]], grad_fn=<DivBackward0>)\n",
      "magnitude of weight_direction:\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "weight_magnitude = torch.tensor([linear.weight[i, :].norm() for i in torch.arange(linear.weight.shape[0])], dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "weight_direction = linear.weight / weight_magnitude\n",
    "\n",
    "print('linear.weight:')\n",
    "print(linear.weight)\n",
    "\n",
    "print('weight_magnitude:')\n",
    "print(weight_magnitude)\n",
    "\n",
    "print('weight_direction:')\n",
    "print(weight_direction)\n",
    "\n",
    "print('magnitude of weight_direction:')\n",
    "print((weight_direction ** 2).sum(dim=-1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputx @ (weight_direction * weight_magnitude).T:\n",
      "tensor([[ 1.0653, -0.4257, -0.5384, -0.1830],\n",
      "        [-0.7710,  0.1401,  1.1635,  0.3304]], grad_fn=<MmBackward0>)\n",
      "linear(inputx):\n",
      "tensor([[ 1.0653, -0.4257, -0.5384, -0.1830],\n",
      "        [-0.7710,  0.1401,  1.1635,  0.3304]], grad_fn=<MmBackward0>)\n",
      "wn_linear(inputx):\n",
      "tensor([[ 1.0653, -0.4257, -0.5384, -0.1830],\n",
      "        [-0.7710,  0.1401,  1.1635,  0.3304]], grad_fn=<MmBackward0>)\n",
      "parameters in wn_linear:\n",
      "weight_g Parameter containing:\n",
      "tensor([[0.7148],\n",
      "        [0.3380],\n",
      "        [0.6460],\n",
      "        [0.2391]], requires_grad=True)\n",
      "weight_v Parameter containing:\n",
      "tensor([[-0.2551, -0.4248, -0.5152],\n",
      "        [ 0.2330,  0.1396,  0.2011],\n",
      "        [-0.2154,  0.2437,  0.5582],\n",
      "        [-0.1916,  0.1428,  0.0081]], requires_grad=True)\n",
      "construct weight of linear:\n",
      "tensor([[-0.2551, -0.4248, -0.5152],\n",
      "        [ 0.2330,  0.1396,  0.2011],\n",
      "        [-0.2154,  0.2437,  0.5582],\n",
      "        [-0.1916,  0.1428,  0.0081]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('inputx @ (weight_direction * weight_magnitude).T:')\n",
    "print(inputx @ (weight_direction * weight_magnitude).T)\n",
    "\n",
    "print('linear(inputx):')\n",
    "print(linear(inputx))\n",
    "\n",
    "print('wn_linear(inputx):')\n",
    "print(wn_linear(inputx))\n",
    "\n",
    "print('parameters in wn_linear:')\n",
    "for n, p in wn_linear.named_parameters():\n",
    "    print(n, p)\n",
    "\n",
    "print('construct weight of linear:')\n",
    "print(wn_linear.weight_g * (wn_linear.weight_v /\n",
    "                            torch.tensor([wn_linear.weight_v[i, :].norm() for i in torch.arange(wn_linear.weight.shape[0])],\n",
    "                                         dtype=torch.float32).unsqueeze(-1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9146fa6d",
   "language": "python",
   "display_name": "PyCharm (DeepLearningTutorial)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}