{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Batch Norm\n",
    "statistic term\n",
    "\n",
    "NLP: [N, L, C] -> [C]\n",
    "\n",
    "CV: [N, C, H, W] -> [C]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.1046, -1.4737,  0.4315, -0.9118],\n         [ 1.2951, -1.8426,  0.0324,  0.3267],\n         [ 0.6766,  1.6677, -0.0437,  1.6402]],\n\n        [[ 1.2373,  0.7291, -1.7653, -0.2277],\n         [ 0.5288, -0.5725,  0.8231, -1.0805],\n         [ 1.1818,  0.2117,  1.8918, -0.1694]]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, time_steps, embedding_dim = 2, 3, 4\n",
    "eps = 1e-5\n",
    "num_groups = 2\n",
    "\n",
    "inputx = torch.randn(batch_size, time_steps, embedding_dim)\n",
    "inputx  # N * L * C"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.6766, -1.0302,  0.1849, -0.9362],\n         [ 1.0473, -1.3318, -0.1782,  0.4419],\n         [-0.3679,  1.5377, -0.2475,  1.9033]],\n\n        [[ 0.9150,  0.7704, -1.8137, -0.1750],\n         [-0.7059, -0.2936,  0.5411, -1.1238],\n         [ 0.7880,  0.3475,  1.5134, -0.1101]]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_norm_op = nn.BatchNorm1d(embedding_dim, affine=False)\n",
    "batch_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.6766, -1.0302,  0.1849, -0.9362],\n         [ 1.0473, -1.3318, -0.1782,  0.4419],\n         [-0.3679,  1.5377, -0.2475,  1.9033]],\n\n        [[ 0.9150,  0.7704, -1.8137, -0.1750],\n         [-0.7059, -0.2936,  0.5411, -1.1238],\n         [ 0.7880,  0.3475,  1.5134, -0.1101]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_mean = inputx.mean(dim=(0, 1), keepdim=True)\n",
    "bn_std = inputx.std(dim=(0, 1), keepdim=True, unbiased=False)\n",
    "\n",
    "(inputx - bn_mean) / (bn_std + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layer Norm\n",
    "常用于NLP\n",
    "\n",
    "statistic term\n",
    "\n",
    "NLP: [N, L, C] -> [N, L]\n",
    "\n",
    "CV: [N, C, H, W] -> [N, H, W]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.7404, -1.3208,  1.1674, -0.5870],\n         [ 1.1804, -1.5791,  0.0699,  0.3288],\n         [-0.4312,  0.9537, -1.4377,  0.9152]],\n\n        [[ 1.0878,  0.6434, -1.5379, -0.1933],\n         [ 0.7751, -0.6380,  1.1527, -1.2898],\n         [ 0.4975, -0.7007,  1.3745, -1.1714]]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm_op = nn.LayerNorm(embedding_dim, elementwise_affine=False)\n",
    "layer_norm_op(inputx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.7404, -1.3208,  1.1674, -0.5870],\n         [ 1.1804, -1.5791,  0.0699,  0.3288],\n         [-0.4312,  0.9536, -1.4377,  0.9152]],\n\n        [[ 1.0878,  0.6434, -1.5379, -0.1933],\n         [ 0.7751, -0.6380,  1.1527, -1.2898],\n         [ 0.4975, -0.7007,  1.3745, -1.1714]]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_mean = inputx.mean(dim=-1, keepdim=True)\n",
    "ln_std = inputx.std(dim=-1, keepdim=True, unbiased=False)\n",
    "(inputx - ln_mean) / (ln_std + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instance Norm\n",
    "常用于风格迁移上\n",
    "\n",
    "statistic term\n",
    "\n",
    "NLP: [N, L, C] -> [N, C]\n",
    "\n",
    "CV: [N, C, H, W] -> [N, C]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.2085, -0.5868,  1.3983, -1.2126],\n         [ 1.2404, -0.8210, -0.5166, -0.0240],\n         [-0.0319,  1.4077, -0.8817,  1.2365]],\n\n        [[ 0.7916,  1.1331, -1.3559,  0.6359],\n         [-1.4106, -1.2994,  0.3299, -1.4119],\n         [ 0.6191,  0.1663,  1.0260,  0.7760]]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins_norm_op = nn.InstanceNorm1d(embedding_dim)\n",
    "ins_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.2085, -0.5868,  1.3983, -1.2126],\n         [ 1.2404, -0.8210, -0.5166, -0.0240],\n         [-0.0319,  1.4077, -0.8817,  1.2365]],\n\n        [[ 0.7916,  1.1331, -1.3559,  0.6359],\n         [-1.4107, -1.2994,  0.3299, -1.4119],\n         [ 0.6191,  0.1663,  1.0260,  0.7760]]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_mean = inputx.mean(dim=1, keepdim=True)\n",
    "in_std = inputx.std(dim=1, keepdim=True, unbiased=False)\n",
    "(inputx - in_mean) / (in_std + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group Norm\n",
    "\n",
    "statistic term\n",
    "\n",
    "NLP: [N, G, L, C // G] -> [N, G]\n",
    "\n",
    "CV: [N, G, C // G, H, W] -> [N, G]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.0252, -1.1699,  0.2446, -1.5256],\n         [ 0.9267, -1.4493, -0.2814,  0.1066],\n         [ 0.4583,  1.2089, -0.3817,  1.8375]],\n\n        [[ 1.1109,  0.2862, -1.4031, -0.1169],\n         [-0.0388, -1.8259,  0.7621, -0.8302],\n         [ 1.0209, -0.5533,  1.6561, -0.0681]]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_norm_op = nn.GroupNorm(num_groups, embedding_dim, affine=False)\n",
    "group_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.0252, -1.1699,  0.2446, -1.5256],\n         [ 0.9267, -1.4492, -0.2814,  0.1066],\n         [ 0.4583,  1.2089, -0.3817,  1.8375]],\n\n        [[ 1.1109,  0.2862, -1.4031, -0.1169],\n         [-0.0388, -1.8259,  0.7621, -0.8302],\n         [ 1.0209, -0.5533,  1.6561, -0.0681]]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_inputxs = torch.split(inputx, split_size_or_sections= embedding_dim // num_groups, dim=-1)\n",
    "results = []\n",
    "for g_inputx in group_inputxs:\n",
    "    gn_mean = g_inputx.mean(dim=(1, 2), keepdim=True)\n",
    "    gn_std = g_inputx.std(dim=(1, 2), keepdim=True, unbiased=False)\n",
    "    gn_result = (g_inputx - gn_mean) / (gn_std + eps)\n",
    "    results.append(gn_result)\n",
    "torch.cat(results, dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weight Norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.6324, -0.6381, -0.1893],\n         [ 1.1542, -1.0547,  0.0614],\n         [-0.5847,  0.3673,  0.4489]],\n\n        [[ 0.5107,  0.0457,  0.1138],\n         [ 0.3587, -0.6211, -0.0026],\n         [-0.0274, -0.7776,  0.3750]]], grad_fn=<UnsafeViewBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(embedding_dim, 3, bias=False)\n",
    "wn_linear = nn.utils.weight_norm(linear)\n",
    "wn_linear(inputx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.6324, -0.6381, -0.1893],\n         [ 1.1542, -1.0547,  0.0614],\n         [-0.5847,  0.3673,  0.4489]],\n\n        [[ 0.5107,  0.0457,  0.1138],\n         [ 0.3587, -0.6211, -0.0026],\n         [-0.0274, -0.7776,  0.3750]]], grad_fn=<MulBackward0>)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_direction = linear.weight / linear.weight.norm(dim=1, keepdim=True)\n",
    "weight_magnitude = wn_linear.weight_g\n",
    "inputx @ (weight_direction.transpose(-1, -2)) * (weight_magnitude.transpose(-1, -2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "关于权重归一化的再次说明"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "batch_size, feat_dim, hid_dim = 2, 3, 4\n",
    "inputx = torch.randn(batch_size, feat_dim)\n",
    "linear = nn.Linear(feat_dim, hid_dim, bias=False)\n",
    "wn_linear = nn.utils.weight_norm(linear)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight:\n",
      "tensor([[ 0.4706,  0.3531, -0.3592],\n",
      "        [ 0.5306, -0.2951, -0.1585],\n",
      "        [-0.5772, -0.1889,  0.0781],\n",
      "        [ 0.3203, -0.5723,  0.1163]], grad_fn=<WeightNormInterfaceBackward0>)\n",
      "weight_magnitude:\n",
      "tensor([[0.6893],\n",
      "        [0.6276],\n",
      "        [0.6124],\n",
      "        [0.6661]])\n",
      "weight_direction:\n",
      "tensor([[ 0.6827,  0.5123, -0.5211],\n",
      "        [ 0.8456, -0.4703, -0.2526],\n",
      "        [-0.9426, -0.3084,  0.1276],\n",
      "        [ 0.4809, -0.8592,  0.1746]], grad_fn=<DivBackward0>)\n",
      "magnitude of weight_direction:\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "weight_magnitude = torch.tensor([linear.weight[i, :].norm() for i in torch.arange(linear.weight.shape[0])], dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "weight_direction = linear.weight / weight_magnitude\n",
    "\n",
    "print('linear.weight:')\n",
    "print(linear.weight)\n",
    "\n",
    "print('weight_magnitude:')\n",
    "print(weight_magnitude)\n",
    "\n",
    "print('weight_direction:')\n",
    "print(weight_direction)\n",
    "\n",
    "print('magnitude of weight_direction:')\n",
    "print((weight_direction ** 2).sum(dim=-1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputx @ (weight_direction * weight_magnitude).T:\n",
      "tensor([[-0.0750,  0.0486, -0.0290,  0.1244],\n",
      "        [-0.5817, -0.3314,  0.5547, -0.0039]], grad_fn=<MmBackward0>)\n",
      "linear(inputx):\n",
      "tensor([[-0.0750,  0.0486, -0.0290,  0.1244],\n",
      "        [-0.5817, -0.3314,  0.5547, -0.0039]], grad_fn=<MmBackward0>)\n",
      "wn_linear(inputx):\n",
      "tensor([[-0.0750,  0.0486, -0.0290,  0.1244],\n",
      "        [-0.5817, -0.3314,  0.5547, -0.0039]], grad_fn=<MmBackward0>)\n",
      "parameters in wn_linear:\n",
      "weight_g Parameter containing:\n",
      "tensor([[0.6893],\n",
      "        [0.6276],\n",
      "        [0.6124],\n",
      "        [0.6661]], requires_grad=True)\n",
      "weight_v Parameter containing:\n",
      "tensor([[ 0.4706,  0.3531, -0.3592],\n",
      "        [ 0.5306, -0.2951, -0.1585],\n",
      "        [-0.5772, -0.1889,  0.0781],\n",
      "        [ 0.3203, -0.5723,  0.1163]], requires_grad=True)\n",
      "construct weight of linear:\n",
      "tensor([[ 0.4706,  0.3531, -0.3592],\n",
      "        [ 0.5306, -0.2951, -0.1585],\n",
      "        [-0.5772, -0.1889,  0.0781],\n",
      "        [ 0.3203, -0.5723,  0.1163]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('inputx @ (weight_direction * weight_magnitude).T:')\n",
    "print(inputx @ (weight_direction * weight_magnitude).T)\n",
    "\n",
    "print('linear(inputx):')\n",
    "print(linear(inputx))\n",
    "\n",
    "print('wn_linear(inputx):')\n",
    "print(wn_linear(inputx))\n",
    "\n",
    "print('parameters in wn_linear:')\n",
    "for n, p in wn_linear.named_parameters():\n",
    "    print(n, p)\n",
    "\n",
    "print('construct weight of linear:')\n",
    "print(wn_linear.weight_g * (wn_linear.weight_v /\n",
    "                            torch.tensor([wn_linear.weight_v[i, :].norm() for i in torch.arange(wn_linear.weight.shape[0])],\n",
    "                                         dtype=torch.float32).unsqueeze(-1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "conv1d = nn.Conv1d(feat_dim, hid_dim, kernel_size=1, bias=False)\n",
    "wn_conv1d = nn.utils.weight_norm(conv1d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters of wn_conv1d:\n",
      "weight_g Parameter containing:\n",
      "tensor([[[0.7590]],\n",
      "\n",
      "        [[0.6893]],\n",
      "\n",
      "        [[0.6723]],\n",
      "\n",
      "        [[0.7030]]], requires_grad=True) torch.Size([4, 1, 1])\n",
      "weight_v Parameter containing:\n",
      "tensor([[[ 0.3721],\n",
      "         [-0.4501],\n",
      "         [ 0.4847]],\n",
      "\n",
      "        [[-0.4077],\n",
      "         [ 0.4126],\n",
      "         [-0.3724]],\n",
      "\n",
      "        [[-0.4312],\n",
      "         [ 0.5121],\n",
      "         [ 0.0611]],\n",
      "\n",
      "        [[-0.3843],\n",
      "         [-0.4427],\n",
      "         [-0.3879]]], requires_grad=True) torch.Size([4, 3, 1])\n",
      "construct weight of conv1d:\n",
      "tensor([[[ 0.3721,  0.4097,  0.4201,  0.4017],\n",
      "         [-0.4501, -0.4956, -0.5082, -0.4860],\n",
      "         [ 0.4847,  0.5337,  0.5473,  0.5233]],\n",
      "\n",
      "        [[-0.3703, -0.4077, -0.4181, -0.3998],\n",
      "         [ 0.3747,  0.4126,  0.4230,  0.4045],\n",
      "         [-0.3383, -0.3724, -0.3819, -0.3652]],\n",
      "\n",
      "        [[-0.3820, -0.4206, -0.4312, -0.4124],\n",
      "         [ 0.4536,  0.4994,  0.5121,  0.4897],\n",
      "         [ 0.0542,  0.0596,  0.0611,  0.0585]],\n",
      "\n",
      "        [[-0.3560, -0.3920, -0.4019, -0.3843],\n",
      "         [-0.4101, -0.4515, -0.4630, -0.4427],\n",
      "         [-0.3593, -0.3956, -0.4057, -0.3879]]], grad_fn=<MulBackward0>)\n",
      "conv1d.weight:\n",
      "tensor([[[ 0.3721],\n",
      "         [-0.4501],\n",
      "         [ 0.4847]],\n",
      "\n",
      "        [[-0.4077],\n",
      "         [ 0.4126],\n",
      "         [-0.3724]],\n",
      "\n",
      "        [[-0.4312],\n",
      "         [ 0.5121],\n",
      "         [ 0.0611]],\n",
      "\n",
      "        [[-0.3843],\n",
      "         [-0.4427],\n",
      "         [-0.3879]]], grad_fn=<WeightNormInterfaceBackward0>)\n",
      "conv1d_weight_magnitude:\n",
      "tensor([0.7590, 0.6893, 0.6723, 0.7030])\n",
      "conv1d_weight_direction:\n",
      "tensor([[[ 0.4903,  0.5398,  0.5535,  0.5293],\n",
      "         [-0.5931, -0.6530, -0.6696, -0.6403],\n",
      "         [ 0.6387,  0.7032,  0.7211,  0.6895]],\n",
      "\n",
      "        [[-0.5372, -0.5915, -0.6065, -0.5800],\n",
      "         [ 0.5436,  0.5985,  0.6137,  0.5868],\n",
      "         [-0.4907, -0.5403, -0.5540, -0.5298]],\n",
      "\n",
      "        [[-0.5682, -0.6256, -0.6415, -0.6134],\n",
      "         [ 0.6747,  0.7429,  0.7618,  0.7284],\n",
      "         [ 0.0806,  0.0887,  0.0910,  0.0870]],\n",
      "\n",
      "        [[-0.5064, -0.5576, -0.5717, -0.5467],\n",
      "         [-0.5833, -0.6423, -0.6586, -0.6298],\n",
      "         [-0.5111, -0.5628, -0.5771, -0.5518]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv1d_weight_magnitude = torch.tensor([conv1d.weight[i, :, :].norm()\n",
    "                                        for i in torch.arange(conv1d.weight.shape[0])], dtype=torch.float32)\n",
    "conv1d_weight_direction = conv1d.weight / conv1d_weight_magnitude\n",
    "\n",
    "print('parameters of wn_conv1d:')\n",
    "for n, p in wn_conv1d.named_parameters():\n",
    "    print(n, p, p.shape)\n",
    "\n",
    "print('construct weight of conv1d:')\n",
    "print(wn_conv1d.weight_g * (wn_conv1d.weight_v / torch.tensor([wn_conv1d.weight_v[i, :, :].norm()\n",
    "                                                               for i in torch.arange(conv1d.weight.shape[0])],\n",
    "                                                               dtype=torch.float32)))\n",
    "\n",
    "print('conv1d.weight:')\n",
    "print(conv1d.weight)\n",
    "\n",
    "print('conv1d_weight_magnitude:')\n",
    "print(conv1d_weight_magnitude)\n",
    "\n",
    "print('conv1d_weight_direction:')\n",
    "print(conv1d_weight_direction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9146fa6d",
   "language": "python",
   "display_name": "PyCharm (DeepLearningTutorial)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}