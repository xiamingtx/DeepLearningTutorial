{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 目标函数大总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# logits shape:[BS, NC]\n",
    "batch_size = 2\n",
    "num_class = 4\n",
    "\n",
    "logits = torch.randn(batch_size, num_class)  # input unnormalized score\n",
    "\n",
    "target_indices = torch.randint(num_class, size=(batch_size,))  # delta目标分布\n",
    "target_logits = torch.randn(batch_size, num_class)  # 非delta目标分布"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CE Loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy loss1: 1.3177180290222168\n",
      "cross entropy loss2: 1.6173157691955566\n"
     ]
    }
   ],
   "source": [
    "# 1. 调用Cross Entropy loss\n",
    "\n",
    "# method 1 for CE loss\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "ce_loss = ce_loss_fn(logits, target_indices)\n",
    "print(f'cross entropy loss1: {ce_loss}')\n",
    "\n",
    "# method 2 for CE loss\n",
    "ce_loss = ce_loss_fn(logits, torch.softmax(target_logits, dim=-1))\n",
    "print(f'cross entropy loss2: {ce_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLL Loss(Negative Log Likelihood loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log-likelihood loss: 1.3177179098129272\n"
     ]
    }
   ],
   "source": [
    "nll_fn = nn.NLLLoss()\n",
    "nll_Loss = nll_fn(torch.log_softmax(logits, dim=-1) + 1e-7, target_indices)\n",
    "# nll_loss = nll_fn(torch.log(torch.softmax(logits, -1)) + 1e-7, target_indices)\n",
    "\n",
    "print(f'negative log-likelihood loss: {nll_Loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "cross entropy value = NLL value\n",
    "\n",
    "### 3. 调用Kullback-Leibler divergence loss (KL loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kullback-leibler divergence loss: 0.09241800010204315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PythonDownLoad\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kl_loss_fn = nn.KLDivLoss()\n",
    "kld_loss = kl_loss_fn(torch.log_softmax(logits, dim=-1), torch.softmax(target_logits, dim=-1))\n",
    "print(f'kullback-leibler divergence loss: {kld_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. 验证 CE = IE + KLD\n",
    "\n",
    "$H(p, q) = H(p) + D_{KL}(P \\| q)$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy loss sample: tensor([1.2337, 2.0009])\n",
      "kullback-leibler divergence loss sample: tensor([0.0774, 0.6619])\n",
      "information entropy sample: tensor([1.1563, 1.3389])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "ce_loss_fn_sample = nn.CrossEntropyLoss(reduction='none')\n",
    "ce_loss_sample = ce_loss_fn_sample(logits, torch.softmax(target_logits, dim=-1))\n",
    "print(f'cross entropy loss sample: {ce_loss_sample}')\n",
    "\n",
    "kld_loss_fn_sample = nn.KLDivLoss(reduction='none')\n",
    "kld_loss_sample = kld_loss_fn_sample(torch.log_softmax(logits, dim=-1), torch.softmax(target_logits, dim=-1)).sum(-1)\n",
    "print(f'kullback-leibler divergence loss sample: {kld_loss_sample}')\n",
    "\n",
    "target_information_entropy = torch.distributions.Categorical(probs=torch.softmax(target_logits, dim=-1)).entropy()\n",
    "print(f'information entropy sample: {target_information_entropy}')  # IE为常数 如果目标分布是delta分布 IE=0\n",
    "\n",
    "print(torch.allclose(ce_loss_sample, kld_loss_sample + target_information_entropy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Binary Cross Entropy loss (BCE Loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary cross entropy loss: 0.8749682903289795\n"
     ]
    }
   ],
   "source": [
    "bce_loss_fn = nn.BCELoss()\n",
    "logits = torch.randn(batch_size)\n",
    "prob_1 = torch.sigmoid(logits)\n",
    "target = torch.randint(2, size=(batch_size, ))\n",
    "bce_loss = bce_loss_fn(prob_1, target.float())\n",
    "print(f'binary cross entropy loss: {bce_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "用NLL Loss 代替BCE Loss 做二分类"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log-likelihood loss binary: 0.8749683499336243\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "prob_0 = 1 - prob_1.unsqueeze(-1)\n",
    "prob = torch.cat([prob_0, prob_1.unsqueeze(-1)], dim=-1)\n",
    "nll_loss_binary = nll_fn(torch.log(prob), target)\n",
    "print(f'negative log-likelihood loss binary: {nll_loss_binary}')\n",
    "\n",
    "print(torch.allclose(bce_loss, nll_loss_binary))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Cosine Similarity loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity loss: 0.49027764797210693\n"
     ]
    }
   ],
   "source": [
    "cosine_loss_fn = nn.CosineEmbeddingLoss()\n",
    "v1 = torch.randn(batch_size, 512)\n",
    "v2 = torch.randn(batch_size, 512)\n",
    "target = torch.randint(2, size=(batch_size,)) * 2 - 1  # -1 或 1\n",
    "cosine_loss = cosine_loss_fn(v1, v2, target)\n",
    "print(f'cosine similarity loss: {cosine_loss}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9146fa6d",
   "language": "python",
   "display_name": "PyCharm (DeepLearningTutorial)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}