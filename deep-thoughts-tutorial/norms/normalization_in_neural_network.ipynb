{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Batch Norm\n",
    "statistic term\n",
    "\n",
    "NLP: [N, L, C] -> [C]\n",
    "\n",
    "CV: [N, C, H, W] -> [C]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.0911, -0.4127,  0.8924,  0.0615],\n         [-0.5815, -1.5187, -0.3445,  0.1576],\n         [-1.5346,  1.6328,  0.6028, -0.6954]],\n\n        [[ 0.2712,  0.1874,  0.7269,  0.0604],\n         [ 0.5891,  0.3546,  0.3073,  0.1127],\n         [ 1.2444,  0.3722, -2.3264,  0.9483]]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, time_steps, embedding_dim = 2, 3, 4\n",
    "eps = 1e-5\n",
    "num_groups = 2\n",
    "\n",
    "inputx = torch.randn(batch_size, time_steps, embedding_dim)\n",
    "inputx  # N * L * C"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.9350, -0.5442,  0.8298, -0.0968],\n         [-0.4098, -1.7123, -0.2907,  0.1052],\n         [-1.3921,  1.6161,  0.5675, -1.6874]],\n\n        [[ 0.4688,  0.0896,  0.6799, -0.0991],\n         [ 0.7965,  0.2661,  0.2998,  0.0110],\n         [ 1.4716,  0.2848, -2.0863,  1.7670]]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_norm_op = nn.BatchNorm1d(embedding_dim, affine=False)\n",
    "batch_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.9350, -0.5442,  0.8298, -0.0968],\n         [-0.4098, -1.7123, -0.2907,  0.1052],\n         [-1.3921,  1.6161,  0.5675, -1.6874]],\n\n        [[ 0.4688,  0.0896,  0.6799, -0.0991],\n         [ 0.7965,  0.2661,  0.2998,  0.0110],\n         [ 1.4716,  0.2848, -2.0863,  1.7670]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_mean = inputx.mean(dim=(0, 1), keepdim=True)\n",
    "bn_std = inputx.std(dim=(0, 1), keepdim=True, unbiased=False)\n",
    "\n",
    "(inputx - bn_mean) / (bn_std + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layer Norm\n",
    "常用于NLP\n",
    "\n",
    "statistic term\n",
    "\n",
    "NLP: [N, L, C] -> [N, L]\n",
    "\n",
    "CV: [N, C, H, W] -> [N, H, W]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.3208, -0.3811,  1.4263,  0.2756],\n         [-0.0160, -1.5565,  0.3736,  1.1989],\n         [-1.2682,  1.3469,  0.4966, -0.5753]],\n\n        [[-0.1603, -0.4936,  1.6529, -0.9990],\n         [ 1.4636,  0.0804, -0.1983, -1.3457],\n         [ 0.8386,  0.2213, -1.6888,  0.6290]]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm_op = nn.LayerNorm(embedding_dim, elementwise_affine=False)\n",
    "layer_norm_op(inputx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-1.3208, -0.3811,  1.4263,  0.2756],\n         [-0.0160, -1.5565,  0.3736,  1.1989],\n         [-1.2682,  1.3469,  0.4966, -0.5753]],\n\n        [[-0.1603, -0.4937,  1.6529, -0.9990],\n         [ 1.4637,  0.0804, -0.1983, -1.3458],\n         [ 0.8386,  0.2213, -1.6888,  0.6290]]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_mean = inputx.mean(dim=-1, keepdim=True)\n",
    "ln_std = inputx.std(dim=-1, keepdim=True, unbiased=False)\n",
    "(inputx - ln_mean) / (ln_std + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instance Norm\n",
    "常用于风格迁移上\n",
    "\n",
    "statistic term\n",
    "\n",
    "NLP: [N, L, C] -> [N, C]\n",
    "\n",
    "CV: [N, C, H, W] -> [N, C]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.0566, -0.2399,  0.9633,  0.5774],\n         [ 1.2520, -1.0871, -1.3783,  0.8293],\n         [-1.1954,  1.3269,  0.4151, -1.4067]],\n\n        [[-1.0622, -1.4079,  0.8566, -0.7705],\n         [-0.2775,  0.5981,  0.5462, -0.6418],\n         [ 1.3396,  0.8098, -1.4028,  1.4122]]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins_norm_op = nn.InstanceNorm1d(embedding_dim)\n",
    "ins_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.0566, -0.2399,  0.9633,  0.5774],\n         [ 1.2520, -1.0871, -1.3783,  0.8293],\n         [-1.1954,  1.3269,  0.4151, -1.4067]],\n\n        [[-1.0622, -1.4087,  0.8566, -0.7705],\n         [-0.2775,  0.5985,  0.5462, -0.6418],\n         [ 1.3396,  0.8103, -1.4028,  1.4122]]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_mean = inputx.mean(dim=1, keepdim=True)\n",
    "in_std = inputx.std(dim=1, keepdim=True, unbiased=False)\n",
    "(inputx - in_mean) / (in_std + eps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group Norm\n",
    "\n",
    "statistic term\n",
    "\n",
    "NLP: [N, G, L, C // G] -> [N, G]\n",
    "\n",
    "CV: [N, G, C // G, H, W] -> [N, G]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.4700,  0.1592,  1.4590, -0.0952],\n         [ 0.0026, -0.8664, -0.8546,  0.0845],\n         [-0.8812,  2.0558,  0.9173, -1.5110]],\n\n        [[-0.6563, -0.8933,  0.7020,  0.0826],\n         [ 0.2433, -0.4203,  0.3121,  0.1312],\n         [ 2.0971, -0.3704, -2.1358,  0.9078]]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_norm_op = nn.GroupNorm(num_groups, embedding_dim, affine=False)\n",
    "group_norm_op(inputx.transpose(-1, -2)).transpose(-1, -2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.4700,  0.1592,  1.4590, -0.0952],\n         [ 0.0026, -0.8664, -0.8546,  0.0845],\n         [-0.8812,  2.0558,  0.9173, -1.5110]],\n\n        [[-0.6563, -0.8933,  0.7020,  0.0826],\n         [ 0.2433, -0.4203,  0.3121,  0.1312],\n         [ 2.0971, -0.3704, -2.1358,  0.9078]]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_inputxs = torch.split(inputx, split_size_or_sections= embedding_dim // num_groups, dim=-1)\n",
    "results = []\n",
    "for g_inputx in group_inputxs:\n",
    "    gn_mean = g_inputx.mean(dim=(1, 2), keepdim=True)\n",
    "    gn_std = g_inputx.std(dim=(1, 2), keepdim=True, unbiased=False)\n",
    "    gn_result = (g_inputx - gn_mean) / (gn_std + eps)\n",
    "    results.append(gn_result)\n",
    "torch.cat(results, dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weight Norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.1737,  0.1331,  0.2459],\n         [ 0.5450,  0.6517, -0.0767],\n         [-1.0854, -0.2878,  0.9288]],\n\n        [[-0.0707, -0.3230, -0.1982],\n         [-0.0475, -0.3142, -0.2021],\n         [ 0.1083,  0.4293,  0.2600]]], grad_fn=<UnsafeViewBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(embedding_dim, 3, bias=False)\n",
    "wn_linear = nn.utils.weight_norm(linear)\n",
    "wn_linear(inputx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.1737,  0.1331,  0.2459],\n         [ 0.5450,  0.6517, -0.0767],\n         [-1.0854, -0.2878,  0.9288]],\n\n        [[-0.0707, -0.3230, -0.1982],\n         [-0.0475, -0.3142, -0.2021],\n         [ 0.1083,  0.4293,  0.2600]]], grad_fn=<MulBackward0>)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_direction = linear.weight / linear.weight.norm(dim=1, keepdim=True)\n",
    "weight_magnitude = wn_linear.weight_g\n",
    "inputx @ (weight_direction.transpose(-1, -2)) * (weight_magnitude.transpose(-1, -2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9146fa6d",
   "language": "python",
   "display_name": "PyCharm (DeepLearningTutorial)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}