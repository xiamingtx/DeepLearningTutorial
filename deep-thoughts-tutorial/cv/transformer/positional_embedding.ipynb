{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. 1d absolute sincos constant embedding\n",
    "Transformer 论文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "n_pos = 4\n",
    "dim = 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def create_1d_absolute_sincos_embedding(n_pos_vec, dim):\n",
    "    # n_pos_vec: torch.arange(n_pos)\n",
    "    assert dim % 2 == 0, 'wrong dimension!'\n",
    "    positional_embedding = torch.zeros(n_pos_vec.numel(), dim, dtype=torch.float)\n",
    "\n",
    "    omega = torch.arange(dim // 2, dtype=torch.float)\n",
    "    omega /= dim / 2.\n",
    "    omega = 1. / (10000 ** omega)\n",
    "    out = n_pos_vec[:, None] @ omega[None, :]\n",
    "\n",
    "    emb_sin = torch.sin(out)\n",
    "    emb_cos = torch.cos(out)\n",
    "\n",
    "    positional_embedding[:, 0::2] = emb_sin\n",
    "    positional_embedding[:, 1::2] = emb_cos\n",
    "    return positional_embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n        [ 0.8415,  0.5403,  0.0100,  0.9999],\n        [ 0.9093, -0.4161,  0.0200,  0.9998],\n        [ 0.1411, -0.9900,  0.0300,  0.9996]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_pos_vec = torch.arange(n_pos, dtype=torch.float)\n",
    "pe = create_1d_absolute_sincos_embedding(n_pos_vec, dim)\n",
    "pe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 1d absolute trainable embedding\n",
    "Vision Transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def create_1d_absolute_trainable_embedding(n_pos_vec, dim):\n",
    "    position_embedding = nn.Embedding(n_pos_vec.numel(), dim)\n",
    "    nn.init.constant_(position_embedding.weight, 0.)\n",
    "\n",
    "    return position_embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 2d relative bias trainable embedding\n",
    "Swin Transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def create_2d_relative_bias_trainable_embedding(n_head, height, width):\n",
    "    # width: 5, [0, 1, 2, 3, 4], bias=[-width+1, width-1], 2 * width - 1\n",
    "    # height: 5, [0, 1, 2, 3, 4], bias=[-height+1, height-1], 2 * height - 1\n",
    "    positional_embedding = nn.Embedding((2 * width - 1) * (2 * height - 1), n_head)\n",
    "    nn.init.constant_(positional_embedding.weight, 0.)\n",
    "\n",
    "    def get_relative_position_index(height, width):\n",
    "\n",
    "        coords = torch.stack(torch.meshgrid(torch.arange(height), torch.arange(width)))  # [2, height, width]\n",
    "        coords_flatten = torch.flatten(coords, 1)  # [2, height * width]\n",
    "        relative_coords_bias = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # [2, height * width, height * width]\n",
    "\n",
    "        # 将bias转换到 >= 0\n",
    "        relative_coords_bias[0, :, :] += height - 1\n",
    "        relative_coords_bias[1, :, :] += width - 1\n",
    "\n",
    "        # A: 2d, B: 1d, B[i * cols + j] = A[i][j]  将二阶张量赋值到一阶\n",
    "        relative_coords_bias[0, :, :] *= relative_coords_bias[1, :, :].max() + 1\n",
    "        return relative_coords_bias.sum(0)  # [height * width, height * width]\n",
    "\n",
    "    relative_position_bias = get_relative_position_index(height, width)\n",
    "    # [height * width, height * width, n_head]\n",
    "    bias_embedding = positional_embedding(torch.flatten(relative_position_bias)).reshape(height * width, height * width, n_head)\n",
    "    bias_embedding = bias_embedding.permute(2, 0, 1).unsqueeze(0)  #[1, n_head, h * w, h * w]\n",
    "    return bias_embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PythonDownLoad\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          ...,\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<UnsqueezeBackward0>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_2d_relative_bias_trainable_embedding(4, 4, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 2d absolute constant sincos embedding\n",
    "Masked AutoEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def create_2d_absolute_sincos_embeddings(height, width, dim):\n",
    "    assert dim % 4 == 0, 'wrong dimensions!'\n",
    "\n",
    "    positional_embedding = torch.zeros(height * width, dim)\n",
    "    coords = torch.stack(torch.meshgrid(torch.arange(height, dtype=torch.float), torch.arange(width, dtype=torch.float)))\n",
    "\n",
    "    height_embedding = create_1d_absolute_sincos_embedding(torch.flatten(coords[0]), dim // 2)  # [h * w, dim / 2]\n",
    "    width_embedding = create_1d_absolute_sincos_embedding(torch.flatten(coords[1]), dim // 2)  # [h * w, dim / 2]\n",
    "\n",
    "    positional_embedding[:, :dim // 2] = height_embedding\n",
    "    positional_embedding[:, dim // 2:] = width_embedding\n",
    "\n",
    "    return positional_embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n        [ 0.0000,  1.0000,  0.8415,  0.5403],\n        [ 0.0000,  1.0000,  0.9093, -0.4161],\n        [ 0.0000,  1.0000,  0.1411, -0.9900],\n        [ 0.8415,  0.5403,  0.0000,  1.0000],\n        [ 0.8415,  0.5403,  0.8415,  0.5403],\n        [ 0.8415,  0.5403,  0.9093, -0.4161],\n        [ 0.8415,  0.5403,  0.1411, -0.9900],\n        [ 0.9093, -0.4161,  0.0000,  1.0000],\n        [ 0.9093, -0.4161,  0.8415,  0.5403],\n        [ 0.9093, -0.4161,  0.9093, -0.4161],\n        [ 0.9093, -0.4161,  0.1411, -0.9900],\n        [ 0.1411, -0.9900,  0.0000,  1.0000],\n        [ 0.1411, -0.9900,  0.8415,  0.5403],\n        [ 0.1411, -0.9900,  0.9093, -0.4161],\n        [ 0.1411, -0.9900,  0.1411, -0.9900]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_2d_absolute_sincos_embeddings(4, 4, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9146fa6d",
   "language": "python",
   "display_name": "PyCharm (DeepLearningTutorial)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}