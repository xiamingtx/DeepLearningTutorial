{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Reference: [Introduction To Conformal Prediction With Python: A Short Guide For Quantifying Uncertainty Of Machine Learning Models](https://www.amazon.com/dp/B0BW2X919P)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from mapie.classification import MapieClassifier\n",
    "from mapie.metrics import classification_coverage_score\n",
    "from mapie.metrics import classification_mean_width_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the data from Excel file\n",
    "bean_data_file = \"datasets/DryBeanDataset/Dry_Bean_Dataset.xlsx\"\n",
    "beans = pd.read_excel(bean_data_file)\n",
    "\n",
    "# Labels are characters but should be integers for sklearn\n",
    "le = LabelEncoder()\n",
    "beans[\"Class\"] = le.fit_transform(beans[\"Class\"])\n",
    "# Split data into classification target and features\n",
    "X, y = beans.drop(\"Class\", axis=1), beans[\"Class\"]\n",
    "\n",
    "# Split of training data\n",
    "X_train, X_rest1, y_train, y_rest1 = train_test_split(X, y, train_size=10000, random_state=2)\n",
    "\n",
    "# From the remaining data, split of test data\n",
    "X_test, X_rest2, y_test, y_rest2 = train_test_split( X_rest1, y_rest1, train_size=1000, random_state=42)\n",
    "\n",
    "# Split remaining into calibration and \"new\" data\n",
    "X_calib, X_new, y_calib, y_new = train_test_split(X_rest2, y_rest2, train_size=1000, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 7.2 The naive method doesn’t work\n",
    "\n",
    "The naive approach would be to take the probabilities at face value and assume they are well calibrated. So, to generate\n",
    "a prediction set with at least $ 1 - \\alpha $ probability, we take the following naive approach: For each data instance,\n",
    "we add up the “probabilities” until the cumulative score of $ 1 - \\alpha $ is exceeded, starting with the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 90.32%\n",
      "Avg. set size: 1.41\n"
     ]
    }
   ],
   "source": [
    "# naive method\n",
    "# Initialize the MapieClassifier\n",
    "mapie_score = MapieClassifier(model, cv=\"prefit\", method=\"naive\")\n",
    "# Calibration step\n",
    "mapie_score.fit(X_train, y_train)\n",
    "# Prediction step\n",
    "y_pred, y_set = mapie_score.predict(X_new, alpha=0.05)\n",
    "# Removing the alpha-dimension\n",
    "y_set = np.squeeze(y_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=red> Note The naive method does not rely on calibration data. So it makes no difference what data you use in\n",
    "mapie_score.fit(). But if you skip this step, you will get an error from MAPIE. </font>\n",
    "\n",
    "Now it’s time to evaluate the method. We will look at the average size of the prediction set, where the smaller the\n",
    "better, as long as the coverage is guaranteed. We will also check the coverage of the prediction sets that we generated\n",
    "for our dataset $X_{new}$, for which we have the labels $y_{new}$.\n",
    "\n",
    "Fortunately, MAPIE already implements functions to compute coverage and set sizes, so we can quickly assess the\n",
    "performance of the naive method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cov = classification_coverage_score(y_new, y_set)\n",
    "setsize = classification_mean_width_score(y_set)\n",
    "print('Coverage: {:.2%}'.format(cov))\n",
    "print('Avg. set size: {:.2f}'.format(setsize))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected, the coverage is too low because the scores in the training data are not suitable for finding the threshold\n",
    "$ \\hat{q}$. The model is too overconfident and therefore the coverage is too low for new data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.3 Score method in MAPIE\n",
    "\n",
    "## 7.3.1 How it works\n",
    "The score method follows the standard conformal prediction recipe. We begin with the training and calibration steps:\n",
    "- Split the data into training and calibration (size $n_{cal}$)\n",
    "- Train the model on the training data\n",
    "- Compute non-conformity scores\n",
    "- Compute the quantile level: $q_{level} = 1 - ceil \\left(\\left(n_{cal}+1 \\right)\\left(1-\\alpha \\right)\\right)/n_{cal}$\n",
    "- Compute the $q_{level}$ quantile $\\hat{q}$ of the scores $s_i$.\n",
    "\n",
    "The quantile level is $1 - \\alpha$ multiplied by $n_{cal} + 1 / n_{cal}$, which serves as a finite sample correction\n",
    "term. The term is only relevant when the calibration dataset is small: If $\\alpha=0.05$ and $n_{cal}=50$,\n",
    "then $q_{level} = 0.97$. If $n_{cal} = 1000$, then $q_{level} = 0.951$.\n",
    "\n",
    "Now we can use this threshold $\\hat{q}$ to create prediction sets for new data:\n",
    "- Get a new sample $x_new$\n",
    "- Compute $s(y, x_{new})$ for all classes $y$\n",
    "- Pick all classes $y$ where $s(y, x_{new}) \\le \\hat{q}$\n",
    "\n",
    "Now we can enjoy our prediction sets with the following marginal coverage guarantee:\n",
    "$$\n",
    "1-\\alpha \\le \\mathbb{P} \\left( Y_{new} \\in C\\left( X_{new} \\right) \\right) \\le 1 - \\alpha + \\frac{1}{n_{cal} + 1}\n",
    "$$\n",
    "Note that there is not only a lower bound, but also an upper bound. The larger the calibration set, the tighter the upper bound.\n",
    "\n",
    "<font color=red> Warning: Don’t use the score method if you need the prediction sets to be adaptive. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 96.28%\n",
      "Avg. set size: 1.83\n"
     ]
    }
   ],
   "source": [
    "# In MAPIE we can use the score method by selecting method='score'\n",
    "mapie_score = MapieClassifier(model, cv='prefit', method='score')\n",
    "mapie_score.fit(X_calib, y_calib)\n",
    "y_pred, y_set = mapie_score.predict(X_new, alpha=0.05)\n",
    "y_set = np.squeeze(y_set)\n",
    "\n",
    "# analyze the coverage and sizes of the prediction sets.\n",
    "cov = classification_coverage_score(y_new, y_set)\n",
    "setsize = classification_mean_width_score(y_set)\n",
    "print('Coverage: {:.2%}'.format(cov))\n",
    "print(\"Avg. set size: {:.2f}\".format(setsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The coverage is now good, given $ \\alpha = 0.05 $, which implies a 95% coverage for the prediction sets. The average set\n",
    "size is also quite small, which is good. But all is not well when using the score method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The score method lacks adaptivity\n",
    "While all conformal prediction methods guarantee marginal coverage, the score method isn’t adaptive to the difficulty of\n",
    "each classification.\n",
    "\n",
    "### Marginal Coverage\n",
    "Marginal coverage means that on average $ 1 - \\alpha $ of the predicted regions contain the true label for new data\n",
    "samples. All conformal predictors guarantee marginal coverage. Marginal coverage can be estimated as the percentage of\n",
    "prediction sets that cover the true class for a new dataset (that is exchangeable with the calibration set).\n",
    "\n",
    "But we have no guarantee that the coverage is $ 1 - \\alpha $ for every data point, not even for groups in the data, such\n",
    "as for every class. Perhaps one type of bean is much harder to classify than another. Then it might well be that the\n",
    "coverage for one class is higher than $ 1 - \\alpha $ and for the other class it’s lower, but on average the coverage is\n",
    "$ 1 - \\alpha $ (= marginal coverage achieved).\n",
    "\n",
    "If you want the conformal predictor to also achieve the coverage guarantee for groups of the data, or even for any point\n",
    "in the feature space, then we speak of conditional coverage.\n",
    "\n",
    "## Conditional Coverage\n",
    "Conditional coverage means that the coverage of $ 1 - \\alpha $ is not only true on average, but also conditional on some\n",
    "feature or grouping of the data. Perfect conditional coverage (broken down to each data point) can’t be guaranteed, only\n",
    "attempted. Conditional coverage for defined groups is possible.\n",
    "\n",
    "Class-conditional coverage is critical for the fictional use case of selling beans with a guarantee. Marginal coverage\n",
    "isn’t sufficient for this use case, since it is possible that one or more varieties are below marginal coverage\n",
    "(and therefore others are above it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class  coverage  avg. set size\n",
      "0  BARBUNYA  0.925287       2.137931\n",
      "1    BOMBAY  1.000000       1.000000\n",
      "2      CALI  0.965909       2.130682\n",
      "3  DERMASON  0.972906       1.463054\n",
      "4     HOROZ  0.938525       1.872951\n",
      "5     SEKER  0.970213       2.017021\n",
      "6      SIRA  0.974277       1.974277\n"
     ]
    }
   ],
   "source": [
    "def class_wise_performance(y_new, y_set, classes):\n",
    "    df = pd.DataFrame()\n",
    "    # Loop through the classes\n",
    "    for i in range(len(classes)):\n",
    "        # Calculate the coverage and set size for the current class\n",
    "        ynew = y_new.values[y_new.values == i]\n",
    "        yscore = y_set[y_new.values == i]\n",
    "        cov = classification_coverage_score(ynew, yscore)\n",
    "        size = classification_mean_width_score(yscore)\n",
    "\n",
    "        # Create a new dataframe with the calculated values\n",
    "        temp_df = pd.DataFrame({\"class\": [classes[i]], \"coverage\": [cov], \"avg. set size\": [size] }, index = [i])\n",
    "        # Concatenate the new dataframe with the existing one\n",
    "        df = pd.concat([df, temp_df])\n",
    "\n",
    "    return df\n",
    "\n",
    "print(class_wise_performance(y_new, y_set, le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The score method is easy to implement and understand. It provides both lower and upper bounds on coverage and produces\n",
    "the smallest average prediction sets compared to other conformal classification methods. However, the method is not\n",
    "adaptive: the coverage guarantee only holds on average. Diﬀicult classification cases may have prediction sets that are\n",
    "too small, and easy cases may have sets that are too large."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Better Recommendation\n",
    "It’s better to use options that are designed to be adaptive. There are several ways to get adaptive prediction sets in\n",
    "classification, and we will learn about all of them in the following sections.\n",
    "- APS and RAPS are adaptive methods\n",
    "- Group-balanced conformal prediction guarantees coverage for groups in the data\n",
    "- Class-conditional conformal prediction guarantees coverage for all classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use Adaptive Prediction Sets (APS) for conditional coverage\n",
    "\n",
    "Before we dive into the theory behind APS, let’s talk more about adaptivity.\n",
    "\n",
    "Imagine you have two clusters in your data. Half of your data is easy to classify, the other half is difficult.\n",
    "A conformal predictor can achieve 90% coverage through the following scenario: The coverage for the prediction sets of\n",
    "the easy data points is 100% and the coverage for the prediction sets of the difficult data points is 80%. The conformal\n",
    "predictor, in this case, is not adaptive. In practice, this means that the prediction sets for the easy data are too\n",
    "large on average, and the prediction sets for the difficult data are too small.\n",
    "\n",
    "Adaptivity: A conformal prediction algorithm is adaptive if it not only achieves marginal coverage, but also\n",
    "(approximately) conditional coverage.\n",
    "\n",
    "## How APS work\n",
    "\n",
    "Let’s remember the score method: The score method uses only the probability of the true label and ignores all other\n",
    "probabilities. For a cat image with probabilities cat=0.3, lion=0.6, and dog=0.1, the non-conformity score $s_i$ would\n",
    "be $s_i = 1 − 0.3 = 0.7$, since we only use the probability of the cat class. APS uses a different non-conformity score:\n",
    "The idea is to add up the probabilities, starting with the largest, down to the true class. So for the example above,\n",
    "the score would be 0.6 + 0.3 = 0.9.\n",
    "\n",
    "Calibration works as usual:\n",
    "- Compute all cumulative scores $s_i$ for the calibration data\n",
    "- Compute the quantile level $q_{level} = 1 - ceil \\left(\\left(n_{cal}+1 \\right) \\alpha \\right)/n_{cal}$\n",
    "- Compute the $q_{level}$ quantile $\\hat{q}$ for the calibration data so that (roughly) $1-\\alpha$ of the scores are below $\\hat{q}$\n",
    "\n",
    "The prediction step also follows the usual recipe:\n",
    "- Compute all class probabilities for a new data point $x_{new}$\n",
    "- Sort the class probabilities in descending order\n",
    "- Include classes, starting with the largest probability, until the threshold $\\hat{q}$ is reached\n",
    "\n",
    "The last step is not well-defined: Do we stop at the class just below the threshold or above it? We have three options\n",
    "regarding the inclusion in the prediction set:\n",
    "- Include the label above the threshold\n",
    "- Don’t include the label above the threshold\n",
    "- Include the label at random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# APS in MAPIE\n",
    "\n",
    "All three inclusion options are implemented in MAPIE. They all start with the same initialization and calibration process."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "MapieClassifier(cv='prefit', estimator=GaussianNB(), method='cumulated_score',\n                random_state=1)",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MapieClassifier(cv=&#x27;prefit&#x27;, estimator=GaussianNB(), method=&#x27;cumulated_score&#x27;,\n                random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MapieClassifier</label><div class=\"sk-toggleable__content\"><pre>MapieClassifier(cv=&#x27;prefit&#x27;, estimator=GaussianNB(), method=&#x27;cumulated_score&#x27;,\n                random_state=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the randomized option, the classifier draws random numbers\n",
    "# So to make the code reproducible, we have to set the random_state\n",
    "mapie_score = MapieClassifier(model, cv=\"prefit\", method=\"cumulated_score\", random_state=1)\n",
    "mapie_score.fit(X_calib, y_calib)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the prediction step, we have to decide whether to include the last label or not. Let’s look at all 3 options,\n",
    "starting with the default of including the label that crosses the threshold.\n",
    "\n",
    "## Last label included\n",
    "\n",
    "In the predict function, we need to specify what to do with the last label that crosses the threshold. In the following\n",
    "code, we include it in the prediction set. The default is to include it, so we could have omitted the\n",
    "'include_last_label' option."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 96.96%\n",
      "Avg. set size: 1.91\n",
      "      class  coverage  avg. set size\n",
      "0  BARBUNYA  0.931034       2.206897\n",
      "1    BOMBAY  1.000000       1.000000\n",
      "2      CALI  0.971591       2.153409\n",
      "3  DERMASON  0.982759       1.512315\n",
      "4     HOROZ  0.946721       1.979508\n",
      "5     SEKER  0.982979       2.114894\n",
      "6      SIRA  0.974277       2.131833\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_set = mapie_score.predict(X_new, alpha=0.05, include_last_label=True)\n",
    "y_set = np.squeeze(y_set)\n",
    "\n",
    "cov = classification_coverage_score(y_new, y_set)\n",
    "setsize = classification_mean_width_score(y_set)\n",
    "print('Coverage: {:.2%}'.format(cov))\n",
    "print(\"Avg. set size: {:.2f}\".format(setsize))\n",
    "print(class_wise_performance(y_new, y_set, le.classes_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis\n",
    "\n",
    "The coverage is slightly above 95%, which is to be expected, since by always including the label above the threshold, we\n",
    "get a larger coverage.\n",
    "\n",
    "The average set size is also larger than for the score method, which is also expected since the score method produces\n",
    "the smallest sets on average.\n",
    "\n",
    "For the class-wise coverage, we have to go into detail:\n",
    "- The coverages are slightly better than for the score method, especially when looking at the lower bounds.\n",
    "- Some coverages are well over 95%, because some classes are super easy to classify and always have a large score for\n",
    "the true class; MAPIE would have to produce empty sets to artificially reduce the coverage, which would be stupid."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Last label excluded\n",
    "Let’s see what happens when we exclude the label."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred, y_set = mapie_score.predict(X_new, alpha=0.05, include_last_label=False)\n",
    "y_set = np.squeeze(y_set)\n",
    "cov = classification_coverage_score(y_new, y_set)\n",
    "setsize = classification_mean_width_score(y_set)\n",
    "print('Coverage: {:.2%}'.format(cov))\n",
    "print(\"Avg. set size: {:.2f}\".format(setsize))\n",
    "print(class_wise_performance(y_new, y_set, le.classes_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The set sizes are rather small, but this comes at a price: The coverage is much lower than the targeted 95%, which is\n",
    "the result of excluding the label that would cross the threshold. In fact, this means that APS with excluded last label\n",
    "is not really a conformal predictor, since it can’t guarantee anything.\n",
    "\n",
    "<font color=red> Warning: Don’t use APS with last label excluded, because it ruins the coverage guarantee.\n",
    "Use it only if you know what you are doing. </font>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Include last label at random\n",
    "Let’s try the last option and include the last label at random."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred, y_set = mapie_score.predict(X_new, alpha=0.05, include_last_label=\"randomized\")\n",
    "y_set = np.squeeze(y_set)\n",
    "cov = classification_coverage_score(y_new, y_set)\n",
    "setsize = classification_mean_width_score(y_set)\n",
    "print('Coverage: {:.2%}'.format(cov))\n",
    "print(\"Avg. set size: {:.2f}\".format(setsize))\n",
    "print(class_wise_performance(y_new, y_set, le.classes_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the coverage is again close to the desired 95%. In fact, of all three inclusion options, the “randomized” option is\n",
    "the only one that has the following guarantee:\n",
    "\n",
    "$$\n",
    "1-\\alpha \\le \\mathbb{P} \\left( Y_{test} \\in C\\left( X_{test} \\right) \\right) \\le 1 - \\alpha + \\frac{1}{n_{cal} + 1}\n",
    "$$\n",
    "\n",
    "This coverage doesn’t hold for the other two options (include_last_label=True/False). The option ‘include_last_label=True’\n",
    "can at least guarantee a coverage above $ 1-\\alpha $.\n",
    "\n",
    "There is another design choice of MAPIE to keep in mind: Of course, one would expect the coverage of the true label to\n",
    "be lowest for “include_last_label=False”, increasing with “randomized”, and highest for “True”. In theory, this is\n",
    "correct. But in practice, this doesn’t always happen. When setting include_last_label to True or False, MAPIE will not\n",
    "produce empty prediction sets. But the “randomized” strategy can produce empty sets. This can lead to a situation where\n",
    "both options (False/True) have a higher than nominal coverage of $ 1-\\alpha $ and therefore higher than the “randomized” option."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion: APS\n",
    "\n",
    "APS can generate adaptive sets. If the randomization option is used, exact marginal coverage can be expected. When many\n",
    "possible class labels are involved, APS can produce large prediction sets and RAPS may be a better option."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Top-k method for fixed size sets\n",
    "\n",
    "A simple alternative is the top-k method (Angelopoulos et al. 2020). The top-k method follows the same conformal\n",
    "classification recipe as APS and the score method, but uses a different non-conformity score. Top-k uses only the rank\n",
    "of the true class instead of the probability outcome. The higher the rank of the true class, the less certain the model\n",
    "classification was. As usual, in the calibration step we find the threshold $\\hat{q}$ for the score, in this case the rank.\n",
    "\n",
    "Using the rank has a drastic effect on the prediction sets: they all have the same size (at least in theory).\n",
    "\n",
    "In MAPIE it’s just a matter of changing the method parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mapie_score = MapieClassifier(model, cv=\"prefit\", method=\"top_k\")\n",
    "mapie_score.fit(X_calib, y_calib)\n",
    "y_pred, y_set = mapie_score.predict(X_new, alpha=0.05)\n",
    "y_set = np.squeeze(y_set)\n",
    "cov = classification_coverage_score(y_new, y_set)\n",
    "setsize = classification_mean_width_score(y_set)\n",
    "print('Coverage: {:.2%}'.format(cov))\n",
    "print(\"Avg. set size: {:.2f}\".format(setsize))\n",
    "print(class_wise_performance(y_new, y_set, le.classes_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The threshold in this case is 3 classes (<font color=red> 3 in the book, but 4 in our experiments </font>). Since the\n",
    "top-k method can only cut at distinct points, the coverage will not be exactly $ 1 - \\alpha $. It would be if we repeated\n",
    "the experiment many times, because sometimes it might also cut at 4 or 2, and then on average it might reach $ 1 - \\alpha $ coverage.\n",
    "\n",
    "But there’s a problem: Why are some set sizes greater than 3? The answer is ties in probabilities. If a bean has the\n",
    "following ordered probabilities: [0.80, 0.17, 0.01, 0.01, 0.01, 0, 0], then 5 classes are included in the set instead of 3,\n",
    "because three classes are tied for third place."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion: Top-k\n",
    "\n",
    "Top-k is even simpler than the Score method and produces fixed size sets. This can be useful if you need to produce the\n",
    "same set size for all data points. Top-k has the worst adaptivity as it literally produces the same size prediction sets\n",
    "no matter how complicated the classification was."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regularized APS (RAPS) for small sets\n",
    "\n",
    "The APS method tends to produce rather large prediction sets, especially if there are more than a handful of possible classes.\n",
    "The reason: there can be a long tail of (noise) classes with low probability. RAPS fixes APS by introducing regularization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How it works\n",
    "\n",
    "RAPS introduces a regularization term that penalizes the inclusion of too many classes. The regularization is based on\n",
    "two new parameters, $\\lambda$ and $k_{reg}$. We won’t go into the details of how the algorithm works, just the intuition\n",
    "behind it.\n",
    "- First, the class probabilities are sorted in descending order.\n",
    "- All classes with a rank higher than $k_{reg}$ get a penalty term.\n",
    "- This penalty depends on $\\lambda$ and how many classes away the true label is from $k_{reg}$\n",
    "- The penalty is added to the class probability.\n",
    "\n",
    "Except for the regularization part, the procedure is the same as the APS procedure. The effect of the penalty is that\n",
    "classes with low probabilities are less likely to be included, because the threshold $\\hat{q}$ is reached earlier. The\n",
    "regularization doesn’t come for free, since you have to sacrifice a percentage of the calibration data to tune the\n",
    "regularization parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAPS in MAPIE\n",
    "\n",
    "As a user of MAPIE, you don’t have to worry about the regularization because it is done automatically.\n",
    "By default, 20% of the calibration data is used for regularization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mapie_score = MapieClassifier(model, cv=\"prefit\", method=\"raps\")\n",
    "mapie_score.fit(X_calib, y_calib, size_raps=0.2)\n",
    "y_pred, y_set = mapie_score.predict(X_new, alpha=0.05, include_last_label=\"randomized\")\n",
    "y_set = np.squeeze(y_set)\n",
    "cov = classification_coverage_score(y_new, y_set)\n",
    "setsize = classification_mean_width_score(y_set)\n",
    "\n",
    "print('Coverage: {:.2%}'.format(cov))\n",
    "print(\"Avg. set size: {:.2f}\".format(setsize))\n",
    "print(class_wise_performance(y_new, y_set, le.classes_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, however, RAPS didn’t really reduce the average size of the prediction set. RAPS is more useful in cases\n",
    "with many classes, such as ImageNet with thousands of classes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion: RAPS\n",
    "\n",
    "RAPS produces smaller prediction sets than APS and is a compromise between Top-k (extreme regularization) and APS (no\n",
    "regularization). In MAPIE, RAPS only works for already trained models (not for CV or LOO). The regularization requires\n",
    "sacrificing some of the calibration data to tune the regularization parameters. RAPS is useful when you have many classes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group-balanced conformal prediction\n",
    "\n",
    "While methods like APS and RAPS can get you a little closer to conditional coverage, they by no means guarantee that\n",
    "coverage will hold for every data point or subset of data points. But what if you have specific groups in the data for\n",
    "which you want to guarantee coverage? Suppose you want the coverage guarantee for image classification to hold for both\n",
    "animals and animals in costumes?\n",
    "\n",
    "Fortunately, group-balanced conformal prediction is not only possible, it is easy and comes with guaranteed coverages\n",
    "per group. Just divide the data into groups and perform the conformal prediction separately for each group. This\n",
    "requires that you know the group at the time of prediction, so group-balanced CP won’t work if we split by class.\n",
    "\n",
    "By doing group-balanced conformal prediction, we get the marginal coverage within each group. Sounds too good to be true!\n",
    "But there is a price to pay: For group-wise CP, we have to split the calibration data by groups. If you have 1000 data\n",
    "points in the calibration set and 10 groups of equal size, then you have only about 100 data points left for calibration\n",
    "per group. As you increase the number of groups, and also if you have unbalanced groups, your calibration data per group\n",
    "will quickly become small. Smaller calibration datasets mean less reliable threshold estimates. While it doesn’t hurt\n",
    "the coverage guarantee (on average), there will be a lot more variance.\n",
    "\n",
    "Let’s try this with the beans. Since all bean features are numeric, we use the area feature to define a group of small beans."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.Area.hist(bins = 100)\n",
    "small_bean_index = (X_calib.Area < 70000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can simply apply CP to the group of small beans."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_calib_group = X_calib[small_bean_index]\n",
    "y_calib_group = y_calib[small_bean_index]\n",
    "mapie_group = MapieClassifier(model, cv=\"prefit\", method=\"score\")\n",
    "mapie_group.fit(X_calib_group, y_calib_group.values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the prediction step, we need to apply the same group definition again to get a subset of the data, and then apply\n",
    "the conformal predictor as usual."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "group_index = (X_new.Area < 70000)\n",
    "X_group = X_new[group_index]\n",
    "y_pred, y_set = mapie_group.predict(X_group, alpha=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could do the same for the large beans. Then we get the marginal coverage guarantee for both groups. Of course, there\n",
    "are more elegant coding solutions for iterating through the groups. This code just shows how group-balanced CP works in\n",
    "principle."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion: group-balanced CP\n",
    "Group-balanced conformal prediction guarantees $ 1 - \\alpha $ coverage for arbitrary groupings of the data. However, the\n",
    "more groups there are, the fewer data points are left in the calibration set per group, and the calibration quality\n",
    "suffers. And the group variable must also be available for new data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class-Conditional APS (CCAPS) for coverage by class\n",
    "\n",
    "Thinking back to the beans use case in the Getting Started chapter, the desired grouping would have been by bean variety.\n",
    "But variety is the outcome to predict by the classification model, so we don’t have access to it at prediction time. If\n",
    "we did, we wouldn’t need our machine learning model.\n",
    "\n",
    "However, there’s still a way to guarantee class-wise coverage: class-conditional conformal prediction.\n",
    "\n",
    "Class-conditional APS (CCAPS) is an approach to guarantee $ 1 - \\alpha $ coverage of prediction sets per class. In many\n",
    "use cases, this is a useful property to have, as it makes the conformal predictor much more adaptive and gives equal\n",
    "attention to each class.\n",
    "\n",
    "The calibration step is the same as for group-balanced conformal prediction, and we use class as the grouping variable.\n",
    "Since the class is known for the calibration set, we can simply split our data by class and apply conformal prediction\n",
    "as we normally would, but by class. For the beans example, we would end up with 7 conformal predictors.\n",
    "\n",
    "The “problem” occurs in the prediction step: we don’t know the true classes for the new data. The solution\n",
    "(Derhacobian et al.): We apply all resulting classwise conformal predictors, and the prediction set is the union of all\n",
    "conformal classes.\n",
    "\n",
    "Let’s say you have $ 𝑘 = 3 $ classes, you pick $ \\alpha = 0.1 $, and the respective thresholds $\\hat{q}_k$ are 0.95, 0.8,\n",
    "and 0.99. Then CCAPS chooses $\\hat{q} = 0.99$. This choice guarantees for all classes that we have a coverage of\n",
    "$ \\ge 1 - \\alpha $, in our case 10%. However, this also means that for some classes the coverage can be much higher than\n",
    "$ 1 - \\alpha $ and thus produce large prediction sets. In this way, for at least one class is the expected coverage at\n",
    "roughly $ 1 - \\alpha $ and the others will automatically be larger, somewhere between 1 and $ 1 - \\alpha $. In our\n",
    "fictional case, they could be 92% for class 1, 98% for class 2, and 90% for class 3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion: class-conditional CP\n",
    "Class conditional CP guarantees $ \\ge 1 - \\alpha $ coverage per class. Like group-wise CP, it reduces the calibration\n",
    "performance by splitting the calibration data into many classes. Also, it can produce rather large prediction sets,\n",
    "since it has to guarantee coverage for the most difficult class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Guide for choosing a conformal classification method\n",
    "\n",
    "This chapter has covered many different conformal classification methods. Which one should you use when?\n",
    "\n",
    "That depends mostly on how much you care about conditional coverage. I have ordered the recommendations from\n",
    "\"don’t care\" to \"it’s important\".\n",
    "\n",
    "- You need fixed size prediction sets ⇒ Use Top-k\n",
    "- You don’t care about conditional coverage at all ⇒ Score method\n",
    "- You don’t care about conditional coverage as long as coverage per group is guaranteed\n",
    "    - If you have enough data per group ⇒ Group balanced conformal prediction\n",
    "    - If you don’t have enough data ⇒ APS or RAPS\n",
    "- You don’t care about conditional coverage as long as each class has guaranteed coverage ⇒ Class conditional conformal prediction\n",
    "- You care about conditional coverage in general\n",
    "    - Not too many classes ⇒ APS\n",
    "    - You want extra small prediction sets and can sacrifice some of the calibration data, or you have not just a handful but hundreds or thousands of classes ⇒ RAPS"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (DeepLearningTutorial)",
   "language": "python",
   "name": "pycharm-9146fa6d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}