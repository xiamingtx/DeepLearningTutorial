{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LSTM与LSTMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义常量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "proj_size = 3\n",
    "\n",
    "input = torch.randn(bs, T, i_size)  # 输入序列\n",
    "c0 = torch.randn(bs, h_size)  # 初始值 不需要训练\n",
    "h0 = torch.randn(bs, h_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 调用官方API"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7186,  0.1966,  0.6559, -0.1657,  0.0615],\n",
      "         [-0.2585, -0.0097,  0.3003, -0.1403,  0.0449],\n",
      "         [-0.3948,  0.0111,  0.1536, -0.1586,  0.0535]],\n",
      "\n",
      "        [[ 0.0563,  0.0184,  0.0143,  0.1093,  0.3215],\n",
      "         [ 0.3571,  0.1254,  0.3315,  0.0475, -0.1405],\n",
      "         [ 0.1477,  0.1639,  0.0257,  0.0627, -0.1628]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "torch.Size([2, 3, 5]) torch.Size([1, 2, 5]) torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "lstm_layer = nn.LSTM(i_size, h_size, batch_first=True)\n",
    "\n",
    "output, (h_final, c_final) = lstm_layer(input, (h0.unsqueeze(0), c0.unsqueeze(0)))\n",
    "\n",
    "print(output)\n",
    "print(output.shape, h_final.shape, c_final.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 5])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "for k, v in lstm_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 实现LSTM模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def lstm_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh):\n",
    "    h0, c0 = initial_states  # 初始状态\n",
    "    bs, T, i_size = input.shape\n",
    "    h_size = w_ih.shape[0] // 4\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1)  # [bs, 4 * h_size, i_size]\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)  # [bs, 4 * h_size, p_size]\n",
    "\n",
    "    output_size = h_size\n",
    "    output = torch.zeros(bs, T, output_size) # 输出序列\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:, t, :]  # 当前时刻的输入向量 [bs, i_size]\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))  # [bs, 4 * h_size, 1]\n",
    "        w_times_x = w_times_x.squeeze(-1)  # [bs, 4 * h_size]\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))  # [bs, 4 * h_size, 1]\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)  # [bs, 4 * h_size]\n",
    "\n",
    "        # 分别计算输入门(i)、遗忘门(f)、cell门(g)、输出门(o)\n",
    "        i_t = torch.sigmoid(w_times_x[:, :h_size] + w_times_h_prev[:, :h_size] + b_ih[:h_size] + b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2 * h_size] + w_times_h_prev[:, h_size:2 * h_size]\n",
    "                            + b_ih[h_size:2 * h_size] + b_hh[h_size:2 * h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2 * h_size:3 * h_size] + w_times_h_prev[:, 2 * h_size:3 * h_size]\n",
    "                            + b_ih[2 * h_size:3 * h_size] + b_hh[2 * h_size:3 * h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:, 3 * h_size:4 * h_size] + w_times_h_prev[:, 3 * h_size:4 * h_size]\n",
    "                            + b_ih[3 * h_size:4 * h_size] + b_hh[3 * h_size:4 * h_size])\n",
    "\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "\n",
    "        output[:, t, :] = prev_h\n",
    "\n",
    "    return output, (prev_h, prev_c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7186,  0.1966,  0.6559, -0.1657,  0.0615],\n",
      "         [-0.2585, -0.0097,  0.3003, -0.1403,  0.0449],\n",
      "         [-0.3948,  0.0111,  0.1536, -0.1586,  0.0535]],\n",
      "\n",
      "        [[ 0.0563,  0.0184,  0.0143,  0.1093,  0.3215],\n",
      "         [ 0.3571,  0.1254,  0.3315,  0.0475, -0.1405],\n",
      "         [ 0.1477,  0.1639,  0.0257,  0.0627, -0.1628]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "output_custom, (h_final_custom, c_final_custom) = lstm_forward(input, (h0, c0), lstm_layer.weight_ih_l0,\n",
    "                                                               lstm_layer.weight_hh_l0, lstm_layer.bias_ih_l0,\n",
    "                                                               lstm_layer.bias_hh_l0)\n",
    "print(output_custom)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 调用官方API"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0429, -0.0155, -0.1125],\n",
      "         [ 0.0399, -0.0240, -0.1188],\n",
      "         [ 0.0226,  0.0233, -0.0486]],\n",
      "\n",
      "        [[ 0.0499,  0.1909,  0.1218],\n",
      "         [-0.0208,  0.0090,  0.1503],\n",
      "         [ 0.0155,  0.0440,  0.1079]]], grad_fn=<TransposeBackward0>)\n",
      "torch.Size([2, 3, 3]) torch.Size([1, 2, 3]) torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "h0 = torch.randn(bs, proj_size)\n",
    "lstmp_layer = nn.LSTM(i_size, h_size, batch_first=True, proj_size=proj_size)\n",
    "\n",
    "output, (h_final, c_final) = lstmp_layer(input, (h0.unsqueeze(0), c0.unsqueeze(0)))\n",
    "\n",
    "print(output)\n",
    "print(output.shape, h_final.shape, c_final.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 3])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n",
      "weight_hr_l0 torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "for k, v in lstmp_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 实现LSTMP"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "def lstmp_forward(input, initial_states, w_ih, w_hh, b_ih, b_hh, w_hr = None):\n",
    "    global batch_w_hr\n",
    "    h0, c0 = initial_states  # 初始状态\n",
    "    bs, T, i_size = input.shape\n",
    "    h_size = w_ih.shape[0] // 4\n",
    "\n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1)  # [bs, 4 * h_size, i_size]\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)  # [bs, 4 * h_size, h_size]\n",
    "\n",
    "    if w_hr is not None:\n",
    "        p_size = w_hr.shape[0]\n",
    "        output_size = p_size\n",
    "        batch_w_hr = w_hr.unsqueeze(0).tile(bs, 1, 1)  # [bs, p_size, h_size]\n",
    "    else:\n",
    "        output_size = h_size\n",
    "    output = torch.zeros(bs, T, output_size) # 输出序列\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:, t, :]  # 当前时刻的输入向量 [bs, i_size]\n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1))  # [bs, 4 * h_size, 1]\n",
    "        w_times_x = w_times_x.squeeze(-1)  # [bs, 4 * h_size]\n",
    "\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1))  # [bs, 4 * h_size, 1]\n",
    "        w_times_h_prev = w_times_h_prev.squeeze(-1)  # [bs, 4 * h_size]\n",
    "\n",
    "        # 分别计算输入门(i)、遗忘门(f)、cell门(g)、输出门(o)\n",
    "        i_t = torch.sigmoid(w_times_x[:, :h_size] + w_times_h_prev[:, :h_size] + b_ih[:h_size] + b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2 * h_size] + w_times_h_prev[:, h_size:2 * h_size]\n",
    "                            + b_ih[h_size:2 * h_size] + b_hh[h_size:2 * h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2 * h_size:3 * h_size] + w_times_h_prev[:, 2 * h_size:3 * h_size]\n",
    "                            + b_ih[2 * h_size:3 * h_size] + b_hh[2 * h_size:3 * h_size])\n",
    "        o_t = torch.sigmoid(w_times_x[:, 3 * h_size:4 * h_size] + w_times_h_prev[:, 3 * h_size:4 * h_size]\n",
    "                            + b_ih[3 * h_size:4 * h_size] + b_hh[3 * h_size:4 * h_size])\n",
    "\n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)  # [bs, h_size]\n",
    "\n",
    "        if w_hr is not None:  # 做projection\n",
    "            prev_h = torch.bmm(batch_w_hr, prev_h.unsqueeze(-1))  # [bs, p_size, 1]\n",
    "            prev_h = prev_h.squeeze(-1)  # [bs, p_size]\n",
    "\n",
    "        output[:, t, :] = prev_h\n",
    "\n",
    "    return output, (prev_h, prev_c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0429, -0.0155, -0.1125],\n",
      "         [ 0.0399, -0.0240, -0.1188],\n",
      "         [ 0.0226,  0.0233, -0.0486]],\n",
      "\n",
      "        [[ 0.0499,  0.1909,  0.1218],\n",
      "         [-0.0208,  0.0090,  0.1503],\n",
      "         [ 0.0155,  0.0440,  0.1079]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "output_custom, (h_final_custom, c_final_custom) = lstmp_forward(input, (h0, c0), lstmp_layer.weight_ih_l0,\n",
    "                                                               lstmp_layer.weight_hh_l0, lstmp_layer.bias_ih_l0,\n",
    "                                                               lstmp_layer.bias_hh_l0, lstmp_layer.weight_hr_l0)\n",
    "print(output_custom)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9146fa6d",
   "language": "python",
   "display_name": "PyCharm (DeepLearningTutorial)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}