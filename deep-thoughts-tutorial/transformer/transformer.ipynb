{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transformer 难点理解与实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### word embedding\n",
    "\n",
    "以序列建模为例, 考虑source sentence 和 target sentence\n",
    "\n",
    "参数设置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# 单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "model_dim = 8  # 原文中是512\n",
    "\n",
    "# 序列的最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "max_position_len = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 构建序列, 序列的字符以其在词表中索引的形式表示"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 1, 0, 0],\n",
      "        [7, 3, 4, 3]])\n",
      "tensor([[5, 7, 3, 7],\n",
      "        [1, 2, 5, 0]])\n"
     ]
    }
   ],
   "source": [
    "# src_len = torch.randint(2, 5, (batch_size, ))\n",
    "# tgt_len = torch.randint(2, 5, (batch_size, ))\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.tensor([4, 3], dtype=torch.int32)\n",
    "\n",
    "# 单词索引构成源句子和目标句子, 构建batch, 并且做了padding 默认值为0\n",
    "src_seq = torch.stack([F.pad(torch.randint(1, max_num_src_words, (L, )), (0, max(src_len) - L)) for L in src_len])\n",
    "tgt_seq = torch.stack([F.pad(torch.randint(1, max_num_tgt_words, (L, )), (0, max(tgt_len) - L)) for L in tgt_len])\n",
    "\n",
    "print(src_seq)\n",
    "print(tgt_seq)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 构造 word embedding  由于padding中默认加了0 所以词表数量 + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.1059e-01, -1.4507e-01, -2.9596e-01,  7.5678e-01, -2.3224e-01,\n",
      "          -1.5946e+00, -5.0547e-01, -3.3037e-01],\n",
      "         [ 5.7515e-01, -1.5071e-01, -4.2411e-02, -3.9152e-01, -1.0968e+00,\n",
      "           1.5585e+00,  5.6138e-01, -1.2438e-01],\n",
      "         [ 2.0126e+00, -5.6317e-01, -6.5935e-01, -6.2743e-01, -1.8382e-01,\n",
      "          -1.1710e+00,  8.7818e-01, -1.9528e+00],\n",
      "         [ 2.0126e+00, -5.6317e-01, -6.5935e-01, -6.2743e-01, -1.8382e-01,\n",
      "          -1.1710e+00,  8.7818e-01, -1.9528e+00]],\n",
      "\n",
      "        [[ 7.3068e-01,  1.4668e-01, -4.6573e-01,  8.8666e-01,  9.1251e-02,\n",
      "           3.1018e-01,  4.9053e-01, -3.5768e-01],\n",
      "         [ 1.0594e+00,  1.7390e-01,  7.5945e-01,  4.3819e-01, -4.1232e-02,\n",
      "           2.1988e-03, -1.7437e+00,  2.7523e-01],\n",
      "         [ 1.1412e-01, -2.7924e+00, -5.5918e-01, -1.3004e+00, -3.2612e-01,\n",
      "           7.7508e-01, -1.3587e+00,  1.4015e+00],\n",
      "         [ 1.0594e+00,  1.7390e-01,  7.5945e-01,  4.3819e-01, -4.1232e-02,\n",
      "           2.1988e-03, -1.7437e+00,  2.7523e-01]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[[ 0.3200, -0.9702,  1.2966,  0.3955, -0.5801,  0.8256,  0.7066,\n",
      "           0.0650],\n",
      "         [-2.2335, -0.6226, -0.3017,  0.2575, -1.2880,  1.2202, -1.1462,\n",
      "          -0.7682],\n",
      "         [ 1.2498,  0.0729,  1.3720, -0.0982,  0.6932, -0.2076,  0.4580,\n",
      "          -0.0061],\n",
      "         [-2.2335, -0.6226, -0.3017,  0.2575, -1.2880,  1.2202, -1.1462,\n",
      "          -0.7682]],\n",
      "\n",
      "        [[-0.5182,  0.3301, -0.3107, -0.3532,  0.4744, -1.8644,  0.7034,\n",
      "          -0.1892],\n",
      "         [-1.0521,  0.7728,  0.1477,  1.4871, -1.8135, -1.0079,  0.8461,\n",
      "          -0.9239],\n",
      "         [ 0.3200, -0.9702,  1.2966,  0.3955, -0.5801,  0.8256,  0.7066,\n",
      "           0.0650],\n",
      "         [-0.2896,  1.6199,  0.2003, -0.0844,  1.0282,  1.2184, -0.8167,\n",
      "          -1.3308]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "src_embedding_table = nn.Embedding(max_num_src_words + 1, model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1, model_dim)\n",
    "\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "\n",
    "print(src_embedding)\n",
    "print(tgt_embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 构造 positional embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "         [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "           9.9920e-01,  4.0000e-03,  9.9999e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "         [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "           9.9920e-01,  4.0000e-03,  9.9999e-01]]])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "         [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "           9.9920e-01,  4.0000e-03,  9.9999e-01]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "         [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "           9.9920e-01,  4.0000e-03,  9.9999e-01]]])\n"
     ]
    }
   ],
   "source": [
    "pos_mat = torch.arange(max_position_len).reshape(-1, 1)\n",
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape(1, -1) / model_dim)\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)\n",
    "\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "\n",
    "src_pos = torch.stack([torch.arange(max_position_len) for _ in src_len]).to(torch.int32)\n",
    "tgt_pos = torch.stack([torch.arange(max_position_len) for _ in tgt_len]).to(torch.int32)\n",
    "\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "print(src_pe_embedding)\n",
    "print(tgt_pe_embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scaled Dot-product Attention\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "进行scaled是为了概率方差不要太大, 雅可比矩阵不要为0, 避免梯度消失\n",
    "\n",
    "softmax 演示"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1979, 0.2085, 0.2066, 0.2037, 0.1833])\n",
      "tensor([3.7366e-03, 6.6135e-01, 2.7055e-01, 6.4361e-02, 1.7657e-06])\n"
     ]
    }
   ],
   "source": [
    "alpha1 = 0.1\n",
    "alpha2 = 10\n",
    "score = torch.randn(5)\n",
    "prob1 = F.softmax(score*alpha1, -1)\n",
    "prob2 = F.softmax(score*alpha2, -1)\n",
    "\n",
    "print(prob1)\n",
    "print(prob2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "jacobian 演示"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1588, -0.0413, -0.0409, -0.0403, -0.0363],\n",
      "        [-0.0413,  0.1650, -0.0431, -0.0425, -0.0382],\n",
      "        [-0.0409, -0.0431,  0.1639, -0.0421, -0.0379],\n",
      "        [-0.0403, -0.0425, -0.0421,  0.1622, -0.0373],\n",
      "        [-0.0363, -0.0382, -0.0379, -0.0373,  0.1497]])\n",
      "tensor([[ 3.7227e-03, -2.4712e-03, -1.0109e-03, -2.4049e-04, -6.5977e-09],\n",
      "        [-2.4712e-03,  2.2397e-01, -1.7893e-01, -4.2565e-02, -1.1677e-06],\n",
      "        [-1.0109e-03, -1.7893e-01,  1.9735e-01, -1.7413e-02, -4.7770e-07],\n",
      "        [-2.4049e-04, -4.2565e-02, -1.7413e-02,  6.0218e-02, -1.1364e-07],\n",
      "        [-6.5977e-09, -1.1677e-06, -4.7770e-07, -1.1364e-07,  1.7657e-06]])\n"
     ]
    }
   ],
   "source": [
    "def softmax_func(score):\n",
    "    return F.softmax(score, -1)\n",
    "\n",
    "jaco_mat1 = torch.autograd.functional.jacobian(softmax_func, score * alpha1)\n",
    "jaco_mat2 = torch.autograd.functional.jacobian(softmax_func, score * alpha2)\n",
    "\n",
    "print(jaco_mat1)\n",
    "print(jaco_mat2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 构造encoder的self-attention mask\n",
    "\n",
    "mask的shape: [batch_size, max_src_len, max_src_len], 值为1或者-inf\n",
    "\n",
    "mask可以理解为一个邻接矩阵, 每一行反应的是一个单词对所有单词的是否有效性"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([2, 4], dtype=torch.int32)\n",
      "tensor([[[False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [ True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "valid_encoder_pos = torch.unsqueeze(torch.stack([F.pad(torch.ones(L), (0, max(src_len) - L)) for L in src_len]), 2)\n",
    "\n",
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "invalid_encoder_pos_matrix = 1 - valid_encoder_pos_matrix\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "\n",
    "print(valid_encoder_pos_matrix)\n",
    "print(src_len)\n",
    "print(mask_encoder_self_attention)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "构造score进行测试"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "score = torch.randn(batch_size, max(src_len), max(src_len))\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "\n",
    "print(masked_score)\n",
    "print(prob)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.8227e+00, -1.6084e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [ 7.6147e-01, -4.9629e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[ 1.6609e+00,  1.9705e+00,  3.4850e-02,  1.0079e+00],\n",
      "         [ 3.9220e-01, -2.8047e-01,  1.2071e+00,  1.1806e+00],\n",
      "         [-6.2551e-01,  1.0623e+00, -1.4678e-01, -1.6200e+00],\n",
      "         [-2.2957e-01, -1.9798e+00,  1.6098e-01, -3.6392e-01]]])\n",
      "tensor([[[0.9687, 0.0313, 0.0000, 0.0000],\n",
      "         [0.7786, 0.2214, 0.0000, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
      "\n",
      "        [[0.3247, 0.4425, 0.0639, 0.1690],\n",
      "         [0.1675, 0.0855, 0.3784, 0.3685],\n",
      "         [0.1192, 0.6444, 0.1923, 0.0441],\n",
      "         [0.2836, 0.0493, 0.4191, 0.2480]]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 构造intra-attention的mask\n",
    "\n",
    "$$Q @ K^T shape:$$ [batch_size, tgt_seq_len, src_seq_len]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.]]])\n",
      "tensor([[[1., 1., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.]]])\n",
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [ True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "valid_decoder_pos = torch.unsqueeze(torch.stack([F.pad(torch.ones(L), (0, max(tgt_len) - L)) for L in tgt_len]), 2)\n",
    "valid_cross_pos_matrix = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "invalid_cross_pos_matrix = 1 - valid_cross_pos_matrix\n",
    "mask_cross_attention = invalid_cross_pos_matrix.to(torch.bool)\n",
    "\n",
    "print(valid_decoder_pos)\n",
    "print(valid_encoder_pos.transpose(1, 2))\n",
    "print(valid_cross_pos_matrix)\n",
    "print(mask_cross_attention)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### decoder self-attention的mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[False,  True,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False, False,  True],\n",
      "         [False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False, False,  True],\n",
      "         [ True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "valid_decoder_tri_matrix = torch.stack([F.pad(torch.tril(torch.ones(L, L)), (0, max(tgt_len) - L, 0, max(tgt_len) - L)) for L in tgt_len])\n",
    "print(valid_decoder_tri_matrix)\n",
    "\n",
    "invalid_decoder_tri_matrix = (1 - valid_decoder_tri_matrix).to(torch.bool)\n",
    "print(invalid_decoder_tri_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-4.4884e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-3.6396e-01, -2.9015e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [ 4.6684e-01,  4.3242e-01,  8.5617e-03, -1.0000e+09],\n",
      "         [ 9.1782e-01, -1.1736e+00, -4.5122e-01, -4.6846e-01]],\n",
      "\n",
      "        [[ 1.3073e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-8.6327e-01, -9.7686e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [ 4.1243e-02,  7.0956e-01, -9.7122e-01, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4816, 0.5184, 0.0000, 0.0000],\n",
      "         [0.3848, 0.3718, 0.2434, 0.0000],\n",
      "         [0.6143, 0.0759, 0.1562, 0.1536]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5284, 0.4716, 0.0000, 0.0000],\n",
      "         [0.3017, 0.5887, 0.1096, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]]])\n"
     ]
    }
   ],
   "source": [
    "score = torch.randn(batch_size, max(tgt_len), max(tgt_len))\n",
    "masked_score = score.masked_fill(invalid_decoder_tri_matrix, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "\n",
    "print(masked_score)\n",
    "print(prob)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 构建self-attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "构建scaled self-attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, attn_mask):\n",
    "    # Shape of Q, K, V: (batch_size * num_head, seq_len, model_dim / num_head)\n",
    "    score = torch.bmm(Q, K.transpose(-2, -1)) / torch.sqrt(model_dim)\n",
    "    masked_score = score.masked_fill(attn_mask, -1e9)\n",
    "    prob = F.softmax(masked_score, -1)\n",
    "    context = torch.bmm(prob, V)\n",
    "    return context"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9146fa6d",
   "language": "python",
   "display_name": "PyCharm (DeepLearningTutorial)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}