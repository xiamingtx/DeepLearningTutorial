{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## RNN原理以及API\n",
    "\n",
    "### PyTorch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1 单向、单层RNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7447,  0.9569, -0.0550],\n",
      "         [ 0.1866, -0.1136, -0.7676]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.1866, -0.1136, -0.7676]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "single_rnn = nn.RNN(4, 3, 1, batch_first=True)\n",
    "input = torch.randn(1, 2, 4)  # bs * sl * fs\n",
    "output, h_n = single_rnn(input)\n",
    "print(output)\n",
    "print(h_n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2 双向、单层RNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5428,  0.3343, -0.3668,  0.4381, -0.5255,  0.6674],\n",
      "         [-0.4299, -0.7727, -0.1020,  0.5534,  0.3776, -0.1801]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-0.4299, -0.7727, -0.1020]],\n",
      "\n",
      "        [[ 0.4381, -0.5255,  0.6674]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bidirectional_rnn = nn.RNN(4, 3, 1, batch_first=True, bidirectional=True)\n",
    "bi_output, bi_h_n = bidirectional_rnn(input)\n",
    "print(bi_output)\n",
    "print(bi_h_n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 实现单向RNN\n",
    "\n",
    "定义常量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "bs, T = 2, 3  # 批大小，输入序列长度\n",
    "\n",
    "input_size, hidden_size = 2, 3  # 输入特征大小, 隐藏层大小\n",
    "input = torch.randn(bs, T, input_size)  # 随机初始化一个输入特征序列\n",
    "h_prev = torch.zeros(bs, hidden_size)  # 初始隐含状态"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6003,  0.6859, -0.7012],\n",
      "         [ 0.7660,  0.7241, -0.6936],\n",
      "         [ 0.7001, -0.5198,  0.5499]],\n",
      "\n",
      "        [[-0.6602,  0.6324, -0.5846],\n",
      "         [-0.4041,  0.7759, -0.8506],\n",
      "         [ 0.2483,  0.4718, -0.6424]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.7001, -0.5198,  0.5499],\n",
      "         [ 0.2483,  0.4718, -0.6424]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "rnn_output, state_final = rnn(input, h_prev.unsqueeze(0))\n",
    "\n",
    "print(rnn_output)\n",
    "print(state_final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def rnn_forward(input, weight_ih, bias_ih, weight_hh, bias_hh, h_prev):\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs, T, h_dim)  # 初始化输出（状态）矩阵\n",
    "\n",
    "    for t in range(T):\n",
    "        x = input[:, t, :].unsqueeze(2)  # 获取当前时刻输入 bs * input_size * 1\n",
    "        w_ih_batch = weight_ih.unsqueeze(0).tile(bs, 1, 1)  # bs * h_dim * input_size\n",
    "        w_hh_batch = weight_hh.unsqueeze(0).tile(bs, 1, 1)  # bs * h_dim * h_dim\n",
    "\n",
    "        w_times_x = torch.bmm(w_ih_batch, x).squeeze()  # bs * h_dim\n",
    "        w_times_h = torch.bmm(w_hh_batch, h_prev.unsqueeze(2)).squeeze()  # bs * h_dim\n",
    "        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n",
    "\n",
    "        h_out[:, t, :] = h_prev\n",
    "\n",
    "    return h_out, h_prev.unsqueeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[ 0.2882, -0.4006],\n",
      "        [-0.4664, -0.2001],\n",
      "        [ 0.4187,  0.2187]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[ 0.3319, -0.1043, -0.4763],\n",
      "        [-0.0804,  0.0795, -0.3260],\n",
      "        [ 0.3775, -0.0891,  0.4441]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.2358,  0.3048, -0.3172], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([-0.0119, -0.1967,  0.1734], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for k, v in rnn.named_parameters():\n",
    "    print(k, v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "将官方API创建的RNN参数放入我们实现的RNN中"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6003,  0.6859, -0.7012],\n",
      "         [ 0.7660,  0.7241, -0.6936],\n",
      "         [ 0.7001, -0.5198,  0.5499]],\n",
      "\n",
      "        [[-0.6602,  0.6324, -0.5846],\n",
      "         [-0.4041,  0.7759, -0.8506],\n",
      "         [ 0.2483,  0.4718, -0.6424]]], grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.7001, -0.5198,  0.5499],\n",
      "         [ 0.2483,  0.4718, -0.6424]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "custom_rnn_output, custom_state_final = rnn_forward(input, rnn.weight_ih_l0, rnn.bias_ih_l0,\n",
    "                                                    rnn.weight_hh_l0, rnn.bias_hh_l0, h_prev)\n",
    "\n",
    "print(custom_rnn_output)\n",
    "print(custom_state_final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 实现双向RNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def bidirectional_rnn_forward(input, weight_ih, bias_ih, weight_hh, bias_hh, h_prev,\n",
    "                              weight_ih_reverse, bias_ih_reverse, weight_hh_reverse,\n",
    "                              bias_hh_reverse, h_prev_reverse):\n",
    "    bs, T, input_size = input.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs, T, h_dim*2)  # 初始化输出（状态）矩阵, 双向是两倍的特征大小\n",
    "\n",
    "    forward_output = rnn_forward(input, weight_ih, bias_ih, weight_hh, bias_hh, h_prev)[0]\n",
    "    backward_output = rnn_forward(torch.flip(input, [1]), weight_ih_reverse, bias_ih_reverse, weight_hh_reverse,\n",
    "                bias_hh_reverse, h_prev_reverse)[0]\n",
    "\n",
    "    h_out[:, :, :h_dim] = forward_output\n",
    "    h_out[:, :, h_dim:] = backward_output\n",
    "\n",
    "    return h_out, h_out[:, -1, :].reshape(bs, 2, h_dim).transpose(0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8440, -0.0569, -0.8015, -0.4325, -0.3314,  0.5379],\n",
      "         [-0.8681,  0.1661, -0.7116, -0.2285, -0.2285,  0.2005],\n",
      "         [-0.9298, -0.1148,  0.1273,  0.9305,  0.8283, -0.2040]],\n",
      "\n",
      "        [[ 0.5569, -0.2113, -0.7327,  0.5457, -0.7249,  0.4445],\n",
      "         [ 0.3946, -0.6107, -0.2400,  0.2806, -0.6004,  0.2373],\n",
      "         [-0.6069, -0.6419, -0.0389,  0.6437,  0.1966,  0.0750]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-0.9298, -0.1148,  0.1273],\n",
      "         [-0.6069, -0.6419, -0.0389]],\n",
      "\n",
      "        [[-0.4325, -0.3314,  0.5379],\n",
      "         [ 0.5457, -0.7249,  0.4445]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bi_rnn = nn.RNN(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "h_prev = torch.zeros(2, bs, hidden_size)\n",
    "bi_rnn_output, bi_state_final = bi_rnn(input, h_prev)\n",
    "\n",
    "print(bi_rnn_output)\n",
    "print(bi_state_final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.5625,  0.4296],\n",
      "        [-0.0916, -0.0864],\n",
      "        [ 0.3384,  0.1789]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[ 0.3780,  0.2811, -0.1564],\n",
      "        [-0.5480,  0.3370,  0.2068],\n",
      "        [ 0.4430, -0.2981, -0.5468]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([-0.4369,  0.1689, -0.4814], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([-0.5010, -0.4525, -0.0309], requires_grad=True)\n",
      "weight_ih_l0_reverse Parameter containing:\n",
      "tensor([[ 0.3608,  0.4691],\n",
      "        [ 0.5268,  0.0841],\n",
      "        [-0.1010, -0.1879]], requires_grad=True)\n",
      "weight_hh_l0_reverse Parameter containing:\n",
      "tensor([[ 0.0084, -0.2417, -0.5126],\n",
      "        [-0.3220,  0.2346, -0.3049],\n",
      "        [ 0.2511, -0.5149,  0.1071]], requires_grad=True)\n",
      "bias_ih_l0_reverse Parameter containing:\n",
      "tensor([0.5699, 0.1016, 0.4650], requires_grad=True)\n",
      "bias_hh_l0_reverse Parameter containing:\n",
      "tensor([ 0.1358,  0.1725, -0.3583], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for k, v in bi_rnn.named_parameters():\n",
    "    print(k, v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8440, -0.0569, -0.8015,  0.9305,  0.8283, -0.2040],\n",
      "         [-0.8681,  0.1661, -0.7116, -0.2285, -0.2285,  0.2005],\n",
      "         [-0.9298, -0.1148,  0.1273, -0.4325, -0.3314,  0.5379]],\n",
      "\n",
      "        [[ 0.5569, -0.2113, -0.7327,  0.6437,  0.1966,  0.0750],\n",
      "         [ 0.3946, -0.6107, -0.2400,  0.2806, -0.6004,  0.2373],\n",
      "         [-0.6069, -0.6419, -0.0389,  0.5457, -0.7249,  0.4445]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[-0.9298, -0.1148,  0.1273],\n",
      "         [-0.6069, -0.6419, -0.0389]],\n",
      "\n",
      "        [[-0.4325, -0.3314,  0.5379],\n",
      "         [ 0.5457, -0.7249,  0.4445]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "custom_bi_rnn_output, custom_bi_state_final = bidirectional_rnn_forward(input, bi_rnn.weight_ih_l0, bi_rnn.bias_ih_l0,\n",
    "                                                                  bi_rnn.weight_hh_l0, bi_rnn.bias_hh_l0, h_prev[0],\n",
    "                                                                  bi_rnn.weight_ih_l0_reverse, bi_rnn.bias_ih_l0_reverse,\n",
    "                                                                  bi_rnn.weight_hh_l0_reverse, bi_rnn.bias_hh_l0_reverse,\n",
    "                                                                  h_prev[1])\n",
    "print(custom_bi_rnn_output)\n",
    "print(custom_bi_state_final)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9146fa6d",
   "language": "python",
   "display_name": "PyCharm (DeepLearningTutorial)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}